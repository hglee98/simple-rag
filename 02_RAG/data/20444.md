# Gemma 3 QAT 모델: 최첨단 AI를 소비자 GPU에 도입


* 지난달 발표한 **Gemma 3**는 최첨단 성능을 제공하는 오픈 AI 모델이며, NVIDIA H100 같은 **단일 고성능 GPU에서도 실행 가능**함
* **QAT(Quantization-Aware Training)** 기법을 적용한 **경량화 버전**을 출시하여 이제 **소비자용 GPU**에서도 실행 가능해짐
* **int4 양자화** 덕분에 **메모리 사용량이 크게 줄어들며**, 성능 저하를 최소화
* QAT 모델은 **RTX 3090, RTX 4060 등 일반 GPU에서도 실행 가능**하며, Hugging Face, Ollama, LM Studio 등에서 바로 사용 가능함
* 커뮤니티 버전의 다양한 **PTQ 모델도 함께 제공**되어 유연한 선택이 가능

---

Gemma 3 소개 및 성능 개요
------------------

* Google이 발표한 최신 오픈 모델 **Gemma 3**는 뛰어난 성능을 가진 **대규모 언어 모델**임
* BF16(16비트 부동소수점) 정밀도로 **NVIDIA H100 GPU에서 실행 가능**하며, 탁월한 **Chatbot Arena Elo 점수** 기록함
* **BF16 사용 이유**는 모델 간 성능 비교를 공정하게 하기 위함으로, 다양한 최적화 방식이 배제된 상태에서 모델 본연의 성능을 비교 가능함

접근성 향상을 위한 QAT 기반 양자화
---------------------

* 기존 대형 모델은 고사양 클라우드 환경이 필요했지만, **소비자용 하드웨어에서도 실행 가능하게 만들기 위해** QAT 기법을 적용함
* **양자화(Quantization)** 는 모델 내부 수치 정밀도를 줄여 **메모리 사용량을 줄이고 실행을 빠르게 함**
* 예: BF16 대신 int4 형식 사용 시 **4배 이상 압축 효과** 발생

QAT를 활용한 품질 유지
--------------

* 단순 후처리 양자화 대신 **QAT(Quantization-Aware Training)** 방식을 사용하여 훈련 중 양자화를 반영함
* 훈련 과정에서 **약 5,000 스텝 동안 비양자화된 체크포인트의 예측 확률을 목표값으로 사용**
* 이 방식으로 Q4\_0 양자화 시 **Perplexity 감소율을 54% 줄이는 성과** 달성

VRAM 사용량의 획기적인 감소
-----------------

* **int4 양자화로 인한 VRAM 절감** 효과가 크며, 모델별 감소폭은 아래와 같음:

  + **Gemma 3 27B**: 54GB → **14.1GB**
  + **Gemma 3 12B**: 24GB → **6.6GB**
  + **Gemma 3 4B**: 8GB → **2.6GB**
  + **Gemma 3 1B**: 2GB → **0.5GB**
* 이 수치는 모델 가중치를 로딩하는 데 필요한 VRAM만 포함되며, 실행 중 필요한 **KV 캐시**는 별도 VRAM을 요구함

다양한 기기에서 실행 가능
--------------

* **Gemma 3 27B (int4)**: \*\*RTX 3090 (24GB VRAM)\*\*에서 로컬 실행 가능
* **Gemma 3 12B (int4)**: \*\*RTX 4060 Laptop (8GB VRAM)\*\*에서도 문제 없이 실행
* **Gemma 3 4B, 1B**: 스마트폰 및 저사양 기기에서도 구동 가능

손쉬운 통합 및 사용
-----------

* QAT 모델은 **다양한 플랫폼 및 도구에서 바로 사용 가능**:

  + **Ollama**: 명령어 한 줄로 실행
  + **LM Studio**: GUI 환경에서 다운로드 및 실행
  + **MLX**: Apple Silicon 기반에서 고효율 추론 지원
  + **Gemma.cpp**: CPU 환경에서 고성능 실행
  + **llama.cpp**: GGUF 포맷으로 손쉬운 통합

Gemmaverse의 커뮤니티 모델
-------------------

* 공식 QAT 모델 외에도 **다양한 커뮤니티 PTQ 모델** 제공
* 주요 기여자: **Bartowski**, **Unsloth**, **GGML**
* 다양한 모델은 **속도, 크기, 품질의 균형을 맞추어 선택 가능**

지금 바로 시작 가능
-----------

* **AI의 대중화를 위한 중요한 단계**로, Gemma 3의 QAT 버전은 누구나 로컬에서 실행 가능
* 실행 방법:
  + PC: [Ollama](https://ollama.com/library/gemma3)
  + 모델 다운로드: [Hugging Face](https://huggingface.co/collections/google/gemma-3-qat-67ee61ccacbf2be4195c265b), [Kaggle](https://www.kaggle.com/models/google/gemma-3/transformers)
  + 모바일 실행: [Google AI Edge](https://developers.googleblog.com/en/gemma-3-on-mobile-and-web-with-google-ai-edge/) 사용
