# 저장소 샘플링: 크기를 모르는 데이터에서 공정한 무작위 추출 방법


* **저장소 샘플링**은 데이터의 크기를 모를 때 공정하게 **무작위 샘플**을 뽑는 독특하고 효율적인 기법임
* 전통적 방법을 사용하면 지원되지 않는 상황들을 효율적으로 해결할 수 있다는 점에서 **실시간 로그 수집** 등 다양한 분야에 활용됨
* 핵심 아이디어는 **새로운 요소가 등장할 때마다 1/n 확률**로 저장 공간을 갱신하여, 모든 요소에 동일한 선택 기회를 제공함
* 여러 개의 샘플을 고를 경우 확률을 k/n로 확장하고, 해당 확률에 따라 무작위적으로 기존 샘플을 대체함
* 이 알고리듬은 적은 메모리 사용으로도 **공정한 샘플링**을 보장하며, 실시간 처리의 효율성과 신뢰성을 높여 줌

---

저장소 샘플링의 개념 및 필요성
-----------------

* **저장소 샘플링**은 전체 크기를 모르는 데이터 집합에서 공정하게 샘플을 추출하는 효율적인 기법임
* 일반적인 경우, 데이터의 크기를 알 때는 무작위 인덱스를 뽑는 방식이 효과적이지만, 크기를 모를 경우에는 이러한 방법이 불가능함
* 선형적으로 도착하는 대량 데이터(예: 로그 스트림)에서 메모리 사용을 제한해야 하며, 동시에 각 데이터가 동일한 확률로 선택될 필요성이 존재함

크기를 아는 경우의 샘플링
--------------

* 제한된 크기의 집합(예: 10장의 카드)에서는 모든 항목을 섞고 앞에서 원하는 만큼 선택하는 **셔플** 방식이 공정성 보장에 적합함
* 컴퓨터의 **배열** 구조를 활용하면, 직접적으로 무작위 인덱스를 선택해 빠르게 샘플을 추출할 수 있음
* 그러나 실제로 수백만 개의 데이터나 크기를 모르는 스트림에서는 이러한 방식이 비효율적임

크기를 모르는 경우의 샘플링 : 문제점 및 필요성
---------------------------

* 1개씩 데이터를 순차적으로 받아보면서 오직 1개만 저장할 수 있고, 이미 지난 데이터를 되돌릴 수 없는 상황이 현실에서 자주 발생함
* 로그 수집 시스템 등에서는 갑작스러운 트래픽 급증이 발생할 수 있으며, 이로 인해 **서버 과부하**를 방지하기 위해 일부만 샘플링해서 보내야 하는 상황이 존재함
* 임의로 첫 몇 개만 선택하는 방식은 모든 항목에 동일한 기회를 주지 않기 때문에 **공정성** 부족 문제를 야기함

저장소 샘플링 알고리듬의 원리
----------------

* 각 데이터가 들어올 때마자 현재까지 받아본 개수 n을 계산하고, 새 데이터가 1/n 확률로 선택되도록 함
* 최초 데이터는 무조건 선택되고, 이후 각 새로운 데이터는 점점 낮은 확률로 기존 데이터를 대체함으로써 **동일 선택 확률**을 유지함
* 마지막까지 저장된 데이터가 전체 중 어느 것이 되어도 확률이 균등해짐
* 동전 던지기가 아니라 1/n의 확률을 사용하는 방식으로 **모든 데이터에 공정한 기회 보장**이 실현됨

수학적 직관 (카드 예시 활용 설명)
--------------------

* 1번째 데이터: 무조건 선택됨 (확률 1/1 임)
* 2번째 데이터: 1/2 확률로 선택, 기존 데이터는 50% 확률만 남음
* 3번째 데이터: 새 데이터는 1/3로 선택, 기존 데이터는 그 확률의 보완값만큼 생존 확률이 누적됨
* 일반화하면, n번째 데이터까지 포함할 때 항상 1/n의 확률을 모든 데이터가 가짐

여러 개의 샘플 선택 확장 (k-out-of-n)
---------------------------

* k개의 샘플을 뽑으려면 **새로운 데이터가 k/n 확률**로 선택되며, 선택 시 현재 저장된 항목 중 랜덤하게 하나를 대체함
* 이 방식 역시 저장되는 모든 항목이 동일 확률로 샘플로 남게 됨
* **일정한 메모리(k만큼)만 사용하면서** 큰 데이터 스트림에서도 공정하게 여러 샘플을 추출 가능함

로그 수집 서비스에서의 저장소 샘플링 활용
-----------------------

* 매 초마다 유입되는 로그 중 최대 k개(예: 5개)만 저장소 샘플링 기법으로 고르고, 해당 샘플만 서버로 전송함
* 데이터가 적을 때는 모든 로그가 전송돼 손실이 없으며, 트래픽 폭증 시에도 k를 초과하여 전송하지 않아 **서비스 안정성 보장**이 가능함
* 일정 주기로 샘플을 보냄으로써 실시간성에서 약간의 **딜레이가 있지만**, 전체적으로 안정성과 비용 효율성 확보에 도움을 줌

추가 응용 및 참고 자료
-------------

* 일부 데이터(예: 에러 로그)가 더 중요하다면 **가중치가 적용된 저장소 샘플링**(Weighted Reservoir Sampling) 변형을 사용할 수 있음
* 심화 개념은 위키피디아 등 외부 자료에 실려 있지만, 기본 원리는 공정성 유지임

결론
--

* 저장소 샘플링은 **크기를 모르는 데이터 스트림**에서 메모리 효율적이고 공정하게 샘플링할 수 있는 매우 **우아하고 실용적인 알고리듬**임
* 실시간 데이터 처리에서 신속성, 일관성, 낮은 자원 사용이라는 장점으로 인해 많은 분야에서 활용 가치가 높음
