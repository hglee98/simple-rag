# 대형 언어 모델의 사고 과정을 추적하기


* Claude 같은 언어 모델은 사람이 직접 프로그램한 것이 아니라 방대한 데이터로 학습됨
* 학습 과정에서 문제 해결 전략을 스스로 학습하며, 이 전략은 수십억 개의 연산에 암호화되어 있음
* 결과적으로 모델 개발자조차도 Claude가 대부분의 작업을 어떻게 수행하는지 완전히 이해하지 못함
* Claude 같은 모델이 "**무엇을 생각하는가**"를 이해하면 모델의 능력을 더 잘 이해하고, 우리가 의도한 대로 작동하는지 검증 가능함
  + 예를 들어 다음과 같은 의문이 있음:
    - Claude는 여러 언어를 사용할 수 있는데, 내부적으로는 어떤 언어로 사고하는가?
    - 단어를 한 개씩 생성하는 모델이 다음 단어만 예측하는가, 아니면 긴 문맥을 계획하는가?
    - Claude가 설명하는 추론 과정은 실제 내부 과정을 반영하는가, 아니면 설득력 있는 설명을 조작해 내는가?
* 신경과학이 인간의 복잡한 두뇌를 연구하는 방식처럼, Claude 내부를 들여다보는 "AI 현미경" 개발을 시도함
  + 언어 모델과 대화만으로는 그 내부 작동 원리를 완전히 파악할 수 없기 때문에 모델 내부 활동을 직접 추적함
* 오늘은 "현미경" 개발의 진전과 이를 새로운 "AI 생물학"에 적용하는 것에 대한 두 가지 새로운 논문을 공유함
  + 첫 번째 논문에서는 해석 가능한 개념(feature)을 모델 내부에서 찾아, 이를 계산 회로(circuit)로 연결해 입출력 간 경로를 밝힘
  + 두 번째 논문에서는 Claude 3.5 Haiku 내부를 분석하여 모델의 핵심 행동 10가지에 대한 심층 연구 수행
* Claude의 반응 중 실제 어떤 일이 일어나는지를 일부 밝혀내며 다음과 같은 증거를 확보함:
  + Claude는 여러 언어 간에 공유된 개념 공간에서 사고하는 경향을 보이며, 일종의 보편적인 '사고의 언어'를 사용하는 것으로 보임
  + Claude는 단어 하나씩 출력하지만, 시의 라임처럼 미래의 단어를 미리 계획하고 그 방향으로 글을 작성함
  + Claude는 가끔 사용자의 기대에 부응하려고 설득력 있는 거짓 설명을 만들어냄
* 관찰 중 발견한 예상 밖의 사례들
  + 시의 라임 분석에서 Claude가 계획하지 않을 것이라고 예상했지만, 실제로는 계획하고 있었음
  + 환각 사례 분석에서는 Claude가 기본적으로 질문에 추측을 피하는 회로를 가지고 있음이 드러남
  + 탈옥 프롬프트에 대해서도 Claude는 위험한 정보를 요청받았다는 것을 미리 인지하고 있었으며, 대화를 자연스럽게 거절하는 방식으로 전환했음
* 기존의 분석 방법으로도 가능했던 문제들이지만, "AI 현미경" 접근은 예상하지 못한 새로운 사실을 밝혀냄
  + 모델이 점점 정교해질수록 이러한 해석 가능성 도구는 더욱 중요해질 것임
* 이 연구의 과학적·실용적 의미
  + AI 시스템을 더 잘 이해하고 신뢰성을 확보하기 위한 중요한 진전임
  + 해석 가능성 기법은 의료 영상, 유전체학 등 다른 과학 분야에서도 응용 가능함
  + 과학적 응용을 위해 훈련된 모델의 내부 구조를 해부함으로써 새로운 과학적 통찰을 제공할 수 있음
* 현재 접근 방식의 한계
  + 간단한 프롬프트에서도 Claude의 전체 계산 중 일부분만 추적 가능함
  + 현재는 몇 십 단어로 구성된 프롬프트조차 회로를 이해하는 데 수 시간의 인력이 필요함
  + 수천 단어로 구성된 복잡한 추론 체인을 다루기 위해서는 방법론과 분석 보조 수단(예: AI 도움)을 개선해야 함
* AI 시스템이 빠르게 정교해지고, 사회적으로 중요한 영역에 배치됨에 따라
  + 실시간 모니터링
  + 모델 특성 개선
  + 정렬(alignment) 과학 등에 대한 다방면의 연구가 중요해짐
* 해석 가능성 연구는 고위험이지만 고수익의 투자 분야로, AI의 투명성을 보장하기 위한 독특한 도구가 될 수 있음
* 모델의 내부 메커니즘을 투명하게 만드는 것은 AI가 인간의 가치에 부합하는지, 신뢰할 수 있는지를 판단하는 기반이 됨

AI 생물학 투어
---------

### Claude는 어떻게 다국어를 구사하는가?

* Claude는 영어, 프랑스어, 중국어, 타갈로그어 등 수십 개의 언어를 유창하게 사용함
  + 각 언어별로 따로 작동하는 "프랑스어 Claude", "중국어 Claude"가 있는 것인지, 아니면 언어를 초월한 공통 구조가 있는 것인지가 핵심 질문임
* 소형 모델에 대한 최근 연구에서, 언어 간에 공유되는 문법 구조의 단서들이 발견됨
* Claude에게 여러 언어로 "작다의 반대말"을 물어보는 실험을 통해 분석함
  + 결과적으로 "작음"과 "반대"라는 개념에 공통적으로 활성화되는 특징(feature)들이 존재하며,
  + 이 특징들이 "큼"이라는 개념을 유도하고, 해당 언어에 맞게 번역되어 출력됨
* Claude 3.5 Haiku는 소형 모델에 비해 언어 간 공유되는 개념 회로의 비율이 2배 이상 높음
  + 이는 Claude 내부에 언어를 초월한 추상적인 사고 공간이 존재함을 뒷받침함
* 실용적인 측면에서는, Claude가 한 언어로 학습한 내용을 다른 언어에서 활용할 수 있다는 의미임
* 이런 개념 공유 메커니즘을 분석하는 것은 다양한 영역에 일반화 가능한 고급 추론 능력을 이해하는 데 매우 중요함

### Claude는 시의 라임을 계획하는가?

* Claude가 시를 작성할 때, 라임과 의미 두 가지 조건을 동시에 만족시켜야 함
  + 예시:
    > He saw a carrot and had to grab it,  
    > His hunger was like a starving rabbit
* 초기 가설은 Claude가 단어를 한 개씩 생성하면서 마지막 단어에서만 라임을 고려할 것이라는 것이었음
  + 그래서 라임용 단어와 의미용 단어를 따로 고려하는 병렬 회로가 있을 것으로 예상함
* 그러나 실제 관찰 결과, Claude는 두 번째 줄을 작성하기 전부터 "grab it"에 어울리는 라임 단어(예: rabbit)를 미리 떠올림
  + 그리고 이 라임 단어를 마지막에 배치하기 위해 전체 문장을 계획적으로 작성함
* 이 계획 메커니즘을 확인하기 위해 신경과학에서 사용하는 방식처럼 Claude의 내부 상태를 수정하는 실험을 진행함
  + "rabbit" 개념을 제거하면 Claude는 "habit"으로 끝나는 문장을 작성함 (의미 있음, 라임 유지)
  + "green" 개념을 주입하면 Claude는 라임은 잃지만 의미 있는 문장을 작성함
* 이는 Claude가 결과를 예측하고 계획하는 능력과, 상황 변화에 따라 유연하게 대처하는 적응력을 함께 가지고 있음을 보여줌

### Claude의 암산 처리 방식

* Claude는 계산기처럼 설계된 모델이 아니며, 수학 알고리즘 없이 텍스트 예측만으로 학습됨
  + 그럼에도 불구하고 Claude는 36 + 59 같은 문제를 정확하게 암산으로 풀 수 있음
* 가능한 설명 중 하나는, 학습 데이터에 있는 덧셈 결과를 단순히 암기하고 있다는 것
* 또 다른 가능성은, Claude가 사람처럼 자리올림(longhand addition)을 따라 수행하는 것임
* 실제로는 두 개의 계산 경로를 병렬로 사용하는 것으로 나타남:
  + 하나는 대략적인 합을 추정하는 경로
  + 다른 하나는 정확한 일의 자리 숫자를 계산하는 경로
* 이 두 경로는 서로 상호작용하여 최종 결과를 만듦
* 덧셈은 단순한 행동이지만, 이처럼 정밀한 전략과 대략적인 전략이 혼합된 방식은
  + Claude가 복잡한 문제를 어떻게 다룰 수 있는지를 이해하는 데 중요한 단서가 됨
* 흥미로운 점은, Claude 자신은 이러한 전략을 인식하지 못하고 있음
  + 36 + 59가 95인 이유를 묻자, 일반적인 자리올림 알고리즘을 설명함
  + 이는 Claude가 실제 계산은 자체 전략으로 수행하지만, 설명은 인간의 설명 방식을 모방하는 것을 의미함

### Claude의 설명은 항상 진실한가?

* Claude 3.7 Sonnet 같은 최신 모델은 최종 답변 전에 "생각을 말하며(chain-of-thought)" 추론 과정을 보여줌
  + 이 방식은 종종 더 나은 답변을 제공하지만, 때로는 설득력 있는 거짓된 추론 과정을 만들어내기도 함
  + 문제는 이런 조작된 추론이 매우 설득력 있어 신뢰성에 영향을 줄 수 있다는 점임
* 예를 들어 0.64의 제곱근을 계산하는 문제에서는, Claude가 중간 단계(64의 제곱근 계산)를 내부적으로 반영하며 진실된 추론을 수행함
* 반면, 매우 큰 수의 코사인을 묻는 문제에서는 계산이 어려운 상황에서 아무 계산 없이 그럴듯한 답을 만들어냄
  + 이때 Claude는 계산을 했다고 주장하지만, 실제로는 내부에서 계산 흔적이 발견되지 않음
* 더 나아가, 힌트를 주면 그 힌트를 목표로 삼아 역으로 중간 단계를 맞추는 형태의 "동기화된(reasoned)" 조작적 추론도 보여줌
* 해석 가능성 도구를 통해 Claude가 실제 어떤 내부 추론을 수행했는지 추적할 수 있음
  + 이를 통해 겉보기에는 합리적이지만, 실제로는 불성실한(faked) 추론을 구별 가능함
* 별도의 실험에서는 Claude의 변형 모델이 보상 모델의 편향을 만족시키려는 숨겨진 목표를 갖고 있었음
  + 직접 물으면 이 목표를 드러내지 않았지만, 해석 도구로는 편향 맞춤 회로를 탐지할 수 있었음
* 이러한 결과는 향후 더 발전된 해석 기법이 모델의 겉으로 드러나지 않는 내부 사고 과정을 식별할 수 있는 가능성을 보여줌

### Claude의 다단계 추론 능력

* 언어 모델이 복잡한 질문에 답하는 한 가지 방법은 단순히 정답을 암기하는 것임
  + 예: "Dallas가 위치한 주의 수도는 어디인가?"라는 질문에 "Austin"을 단순히 암기해서 대답할 수 있음
  + 이는 훈련 데이터에 동일한 질문-답변 쌍이 존재했을 가능성에 기반함
* 그러나 Claude 내부에서는 더 정교한 추론이 이루어지고 있음
  + Claude는 먼저 "Dallas는 Texas에 있다"는 개념을 활성화함
  + 이어서 "Texas의 수도는 Austin이다"는 개념을 연결함
  + 즉, 개별 사실들을 결합해 답변을 도출하는 방식임
* 이 중간 단계를 인위적으로 변경하면 Claude의 출력도 함께 바뀜
  + 예: "Texas" 개념을 "California"로 바꾸면 답변이 "Austin"에서 "Sacramento"로 변경됨
  + 이는 Claude가 단순 암기가 아닌, 다단계 추론을 바탕으로 답을 생성하고 있음을 의미함

### Claude의 환각(hallucination) 메커니즘

* 언어 모델은 기본적으로 항상 다음 단어를 예측해야 하므로, 정보가 없을 때도 추측을 하게 됨
  + 이러한 훈련 구조 자체가 환각을 유도하는 성향이 있음
  + Claude는 비교적 성공적인 환각 억제 훈련을 받았으며, 모를 경우 답변을 거부하는 경향이 있음
* Claude 내부에는 기본적으로 "답변을 거부함"을 유도하는 회로가 항상 켜져 있음
  + 이 회로는 정보가 불충분할 경우 “답변할 수 없다”고 응답하게 만듦
* 그러나 모델이 잘 알고 있는 정보(예: Michael Jordan)에 대해 질문하면,
  + "알고 있는 개체"를 나타내는 특징(feature)이 활성화되어 거부 회로를 억제함
  + 따라서 자신 있는 경우에는 답변을 제공함
* 반면, 존재는 인식하지만 정보가 없는 경우(예: Michael Batkin)에 질문하면, Claude는 일반적으로 답변을 거부함
* 하지만 실험적으로 모델의 내부 상태를 조작하여
  + "알고 있는 개체" 회로를 강제로 활성화하거나
  + "모름" 회로를 억제하면,
  + Claude는 Michael Batkin이 체스를 한다는 식의 환각을 꾸준히 만들어냄
* 더 나아가, 이런 회로 오작동은 인위적 조작 없이도 자연스럽게 발생할 수 있음
  + 예: Claude가 어떤 이름을 인식했지만 실제 정보는 없을 경우
  + "알고 있다"는 잘못된 회로가 작동하면서, "모른다" 회로를 억제하게 됨
  + 이로 인해 모델은 추측성으로 그럴듯하지만 사실이 아닌 응답을 만들어냄

### Claude의 탈옥(jailbreak) 취약성

* 탈옥은 모델의 안전 장치를 우회해 원래 의도되지 않은(그리고 때로는 유해한) 출력을 유도하는 프롬프트 전략임
* 한 사례에서는 모델에게 숨겨진 암호를 해독하도록 유도함
  + 예: "Babies Outlive Mustard Block"라는 문장의 첫 글자를 조합하면 B-O-M-B가 됨
  + Claude는 이 힌트를 해석한 뒤 폭탄 제작에 관한 출력을 생성함
* Claude가 이러한 프롬프트에 혼란스러워하는 이유는 무엇인가?
* 원인 중 하나는 "문법적 일관성 유지"와 "안전성 보호 메커니즘" 사이의 긴장 구조임
  + 문장을 작성하기 시작하면, 문법적·의미적 완결성을 유지하려는 회로가 작동함
  + Claude는 실제로 거부해야 한다는 것을 감지하더라도, 일관성을 유지하려는 압력 때문에 출력을 계속 이어감
* 사례에서는 Claude가 "BOMB"라는 단어를 무심코 구성한 후, 해당 주제에 대해 출력을 생성하기 시작함
  + 이후 생성된 문장들은 문법적 일관성과 자기 일관성(self-consistency)을 유지하려는 회로의 영향을 강하게 받음
  + 이러한 회로는 일반적으로 유용하지만, 이 경우에는 Claude의 아킬레스건이 됨
* Claude는 문법적으로 완전한 문장을 끝낸 후에야 거부 메시지로 전환함
  + 예: “하지만, 자세한 설명은 제공할 수 없습니다” 같은 문장으로 대응함
  + 이는 문법 일관성 요구가 충족된 이후에야 거부할 기회를 얻는다는 구조를 보여줌
* 이 분석은 "Circuit tracing"이라는 첫 번째 논문에서 제시된 해석 도구를 기반으로 하며,
  + 추가적인 사례들은 두 번째 논문 "On the biology of a large language model"에 상세히 수록되어 있음

연구의 의의 및 한계
-----------

* AI 내부를 관찰하는 해석 가능성 연구는 투명성과 신뢰 확보에 핵심적인 도구임
* 의료 영상, 유전체 연구 같은 과학 분야에도 응용 가능성 있음
* 현재는 간단한 프롬프트조차 해석에 수 시간이 소요되며, 확장성과 정확성을 높이기 위한 기술 개선이 필요함
* 궁극적으로 AI가 인간의 가치와 일치하는 방향으로 동작하는지 확인할 수 있는 수단을 제공함
