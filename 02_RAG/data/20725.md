# Low-Bit LLM을 위한 상용 DRAM에서 구현된 매트릭스-벡터 곱셈


* MVDRAM은 수정되지 않은 DRAM을 사용하여 저비트 LLM 추론을 위한 GeMV 연산을 가속화하는 시스템임
* DRAM을 GeMV 엔진으로 활용하여 높은 처리량을 제공함
* 기존 PUD 접근 방식의 입력 사전 배열 및 출력 비트 전환 비용을 제거함
* 실험 결과, 저비트 LLM에서 프로세서 기반 구현보다 뛰어난 성능을 보임
* AI 하드웨어의 새로운 가능성을 제시함

---

MVDRAM: 수정되지 않은 DRAM을 활용한 저비트 LLM 가속
------------------------------------

* **GeMV 연산**은 대형 언어 모델(LLM) 추론에서 중요한 지연 병목 현상으로 남아 있음
* \*\*Processing-Using-DRAM (PUD)\*\*는 DRAM을 GeMV 엔진으로 재활용할 수 있는 잠재력을 가짐
* 그러나 PUD를 LLM 추론 파이프라인에 적용하면 상당한 오버헤드가 발생함

MVDRAM의 혁신적인 접근
---------------

* MVDRAM은 **데이터 공유 패턴**과 **수학적 선형성**을 활용하여 프로세서와 DRAM을 조율함
* 기존 PUD 접근 방식의 비용을 제거하여 GeMV 연산을 가속화함

실험 결과
-----

* 네 개의 DDR4 DRAM 모듈을 사용한 실험에서 MVDRAM은 저비트(4비트 이하) LLM에서 프로세서 기반 구현보다 뛰어난 성능을 보임
* 최대 7.29배의 속도 향상과 30.5배의 에너지 효율성을 달성함

LLM 추론의 전반적인 개선
---------------

* 2비트 및 4비트 양자화 저비트 모델에서 각각 2.18배 및 1.31배의 처리량 개선을 보임
* 에너지 효율성도 각각 3.04배 및 2.35배 향상됨

AI 하드웨어의 새로운 가능성
----------------

* MVDRAM은 표준 DRAM을 LLM 가속기로 활용할 수 있는 가능성을 입증함
* AI 하드웨어의 새로운 지평을 열 수 있는 잠재력을 가짐
