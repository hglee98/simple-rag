# 지속적인 사고 기계


* **연구진은 뇌의 신경 세포가 계산에 있어 타이밍과 동기화를 활용함**이 현대 AI에서 간과된 핵심임을 지적함
* **Continuous Thought Machine(CTM)** 구조를 도입하여 동물 뇌의 **시간 기반 신경 다이나믹스**를 실제 모델에 접목함
* **CTM은 비동기적 내부 사고 차원, 개별 뉴런 단위 모델, 그리고 뉴런 간 동기화 표현**을 활용해 정보 처리함
* 다양한 실험에서 **적응적 계산력, 신경 동기화 기반 기억력, 강한 일반화 능력**을 확인함
* **CTM 구조의 해석 용이성, 생물학적 개연성, 다양한 작업 적합성**을 실증함

---

tl;dr
-----

* 뇌의 뉴런이 계산에 사용하는 **타이밍과 동기화** 특성이 생물 지능의 **유연성**과 적응력의 핵심임
* 현대 AI는 효율성과 단순함을 위해 **이러한 시간 기반 특성**을 버리고 있음
* 연구팀은 **뉴런의 타이밍**이 중요한 생물학적 개연성과 현대 AI의 효율적인 구현 사이의 간극을 좁히는 방안을 찾았음
* 이 결과는 매우 **의외적이며 유망함**을 보여줌

Introduction
------------

* **Neural Network**(NN)은 원래 생물학적 뇌에서 영감을 받았으나, 오늘날의 NNs는 **실제 뇌와 매우 다른 구조**와 다이나믹스를 가짐
* 현대 NN은 **시간적 다이나믹스를 생략**함으로써 대규모 딥러닝을 가능하게 했지만 이는 생물학적 기반에서 벗어난 것임
* 뇌는 **스파이크 타이밍 의존적 가소성(STDP)** 와 뉴런 동기화 등 복잡한 신경 다이나믹스를 활용함
* 이러한 시간적 처리 원리는 **현대 AI에 부족**하여, 사람 수준의 유연한 지능으로 발전하는 데 장애로 작용함
* 따라서 **시간 처리 기능**이 인공지능의 핵심 요소가 되어야 함

Why do this research?
---------------------

* 현대 AI의 높은 성능에도 불구하고, **유연한 인간 인지와 일반성에서 본질적 차이**가 존재함
* 인공지능이 인간 뇌 이상의 성취를 이루기 위해서는 **신경 활동과 타이밍**을 적극적으로 모사해야 함
* 본 연구에서는 **Continuous Thought Machine(CTM)** 을 통해 뉴런 타이밍을 **핵심적 요소**로 도입함
* 주요 기여는 내부 사고 차원의 분리, 개별 뉴런 단위의 신경 모델, 그리고 **동기화 기반 표현 구조**임

Reasoning models and recurrence
-------------------------------

* AI는 점점 단순 입력-출력 매핑에서 벗어나 적극적 **추론 모델**로 진화하고 있음
* 기존 RNN류 순환구조는 최근 Transformer로 대체되었으나, **순환성 자체가 모델 복잡성 확장에 유용**함
* 현대 텍스트 생성 모델 등은 **테스트 시간**에 중간 생성(Recurrence)을 사용하며, 이는 추가 계산과 유연성을 제공함
* **CTM은 기존 방식과 다르게** 내부 분리 점진 사고 차원, 개별 뉴런 수준의 타이밍, 동기화 자체를 과제 해결 표현으로 활용함

Method
------

### 구조 개요

* **CTM은 데이터에 대해 내부적으로 신경 활동이 펼쳐지는 구조**임
* 각 단계마다 ‘pre-activation’ 이력을 수집해 **Neuron Level Model(NLM)** 에 입력함
* 여러 뉴런의 ‘post-activation’ 이력들을 바탕으로 **뉴런 동기화 행렬**을 계산하여 강력한 **동기화 표현** 생성
* 동기화 표현은 **모델의 관찰과 예측의 핵심적 잠재 벡터**로 쓰임

### 세부 구조

1. Internal recurrence(내부 순환)
-----------------------------

* 내부 순환 차원을 활용해 **생각의 진행**이 펼쳐지는 차원을 따로 둠
* 각 내부 tick은 외부 시계열 데이터와 관계없이 **자체 사고 단위로 작동함**

2. Neuron-level models(뉴런 단위 모델)
--------------------------------

* 각 뉴런은 **개인화된 MLP 구조**를 가지며, ‘pre-activation’의 짧은 이력을 입력받아 ‘post-activation’을 만듦

3. Synchronization as representation(동기화 표현)
--------------------------------------------

* 일정 기간 내 모든 ‘post-activation’으로 **뉴런 동기화 행렬**을 계산해, 이를 **핵심적 잠재 표현/행동 벡터**로 삼음

### 입력 데이터와의 관계

* 데이터는 **내부 순환과 동기화** 중심의 처리 방식을 보완적으로 사용
* **동기화 상태에 따라 입력 데이터 관찰 및 예측**이 이루어짐

### Internal ticks: 사고 차원

* **CTM은 자체 사고 타임라인**을 갖고, 데이터 순서와 무관하게 내부에서 반복적으로 정보를 갱신하고 정제함
* 이 차원에서 **지능적 활동의 전개**가 발생함

### Recurrent weights: Synapses

* **U-NET 스타일 MLP**를 통해 ‘pre-activation’을 산출하고, M개 최근 값을 유지함
* 각 뉴런은 **개별 MLP**로 이력 벡터(‘pre-activation’ 시계열)를 받아 ‘post-activation’을 산출함

### Synchronization as a representation

* **뉴런간 동기화 행렬**로 모델이 외부와 상호작용
* 동기화 값은 실제 행동 지표(출력, 관찰, attention query 등)에 직접 사용됨
* 모델 폭 D가 커질수록 **표현력과 정보량이 제곱적으로 증가**하는 특징을 가짐
* attention 등 입력 데이터 모듈과 결합해 더욱 **강력한 정보 처리 능력**을 보임

### Loss function

* 각 내부 tick마다 출력을 산출하고, 그에 해당하는 **loss와 확신도(1-정규화 엔트로피)** 를 계산함
* 전체 loss는 **최소 손실 시점과 최대 확신 시점**을 동적으로 집계하여, 문제 난이도에 맞춘 **적응적 학습**을 유도함

Experiment: ImageNet
--------------------

### Demonstrations

* CTM은 **이미지 데이터에 대해 다양한 attention head와 신경 동기화**를 활용해 예측함
* **정확도, calibration, 확신 임계값 별로 다양한 지표**를 시각화함

### Results

* CTM은 **adaptive compute**를 통해 사고 단계를 조절하며, 일정 단계 이후 추가 benefit이 소폭임을 관찰함
* **16개 attention head**, 각 단계별 class 예측/정확도 및 neuron activity를 함께 시각화함

### Discussion

* CTM은 **데이터와의 직관적이고 유연한 상호작용**을 강조함
* **뉴런 동기화 기반의 표현**을 통해 시각 인식에서도 기존 방식과 명확히 구분됨
* **시간(TIME)** 요소가 사람들이 정보를 처리하는 방식과 근본적으로 맞닿아 있음을 시사함

Experiment: Solving 2D Mazes
----------------------------

### The why and the how

* 2D 미로 풀기는 **도구가 없으면 신경망 모델에 매우 어려운 과제**임
* CTM은 **직접 경로 예측**(L/R/U/D/W) 방식으로 학습되어, attention 패턴이 의도적으로 실제 경로와 일치함
* **일반화 테스트**에서 복잡하고 긴 미로도 높은 정확성/일반화로 풀어냄

### Results & Discussion

* CTM은 가장 긴 경로에서도 **기존 baseline 대비 압도적인 성능**을 보임
* **인간과 유사한 전략적 내부 world model**을 형성해, 단순 암기가 아닌 진짜 reasoning 능력 보유임

### A World Model

* position encoding 없이도 **시각 정보만으로 내부 환경 모델**을 생성하여 문제를 해결함

Experiment: Parity
------------------

* **이진 시퀀스의 중첩된 패리티**(짝수/홀수 합)를 전체 입력 제공 조건에서 예측하도록 훈련함
* 75단계 이상의 내부 사고 tick 사용 시 CTM은 100% 정확도에 도달 가능함
* LSTM은 내부 사고 tick이 많아지면 학습이 불안정해짐

### Learning sequential algorithms

* **attention head의 움직임과 뉴런 활성 패턴**에서, CTM이 데이터를 역방향/정방향 순회 전략을 각자 습득
* 이는 **전략적 계획(Planning) 및 단계적 실행** 역량 증거임

Experiment: Q&A MNIST
---------------------

### Memory via Synchronization

* **MNIST Q&A 과제**로 CTM의 장기 기억/인출 가능성을 테스트함
* 입력 이미지가 뉴런 활성 이력 윈도우를 벗어나도 **동기화로 장기 기억 정보**를 보관/인출함

### Results & Generalization

* **내부 사고 tick 수 증가에 따라 성능이 개선**되며, 복잡한 질문/길이에 대한 **일반화 능력** 탁월함
* LSTM은 더 많은 tick에서 불안정하고, CTM은 일관적으로 학습/추론함

Additional experiments
----------------------

### CTM versus humans

* CIFAR-10에서 **인간, 피드포워드, LSTM과 CTM**의 성능 비교
* **Calibration**(확률 예측 일치도)은 CTM이 인간보다 뛰어남
* 신경 동기화 다이나믹스가 기존 방식과 달리 매우 **다양하고 복잡한 내부 특성**을 나타냄

### CIFAR-100, ablation studies

* 모델 폭이 넓을수록 **뉴런의 다양성/다이나믹스 증가** 관찰
* 내부 tick 수에 따라 **문제에 따라 서로 다른 내부 사고 과정**(‘두 개의 봉우리’ 분포)이 드러남

### Sorting real numbers

* 30개 실수 정렬 실험에서, CTM은 **값 사이의 거리/간극에 따라 내부 계산시간(기다림 tick)이 달라지는** emergent behavior를 보임

### Reinforcement Learning

* **MiniGrid, CartPole 등 RL 환경**에서, CTM은 내부 지속적 사고 단위를 활용해 **환경과의 상호작용 및 정책 결정 수행**
* LSTM과 비슷한 종단 성능을 보이며, **연속적인 사고 기록을 통한 효과**를 입증함

Conclusion
----------

* **CTM은 생물학적 개연성과 AI 효율성의 융합**을 새로운 방식으로 달성함
* **뉴런 단위 모델 도입, 신경 동기화 기반의 새로운 표현 방식**을 활용해 그간 볼 수 없었던 표현 역량을 실현함
* 이미지 분류, 미로 풀기, 기억, 정렬, RL 등 **다양한 작업에 구조 일관성**과 높은 적응력을 보여줌
* **뇌과학과 머신러닝이 가지는 시너지**와, 시간-동기화 중심의 사고 기계 설계의 중요성을 실증함
