# 내 LLM CLI 도구가 이제 Python 코드나 플러그인에서 툴을 실행할 수 있음


* **LLM 0.26**에서는 다양한 **툴 실행 기능**이 추가되어, OpenAI, Anthropic, Gemini, Ollama 등 여러 LLM 모델이 Python 함수나 플러그인으로 구현된 도구를 활용할 수 있음
* 명령줄이나 Python API에서 **툴을 직접 설치하거나 함수로 전달**할 수 있어 확장성과 유연성이 크게 향상됨
* 툴 플러그인으로 **수학 계산, JavaScript 실행, SQL 질의, 외부 서비스 연동** 등 다양한 기능이 모델에 손쉽게 추가 가능함
* **모든 주요 LLM 벤더 및 로컬 모델**에서 툴 호출 패턴이 보편화되고, LLM 0.26도 표준화된 방식으로 통합함
* **향후 계획 및 발전 방향**으로는 MCP 표준 지원, 플러그인 개발 생태계 확장, 툴 실행 로그 개선 등이 논의됨

---

LLM 0.26: 터미널에서 대형언어모델로 툴 실행 지원
-------------------------------

2025년 5월 27일 기준으로 **LLM 0.26** 버전이 출시됨. 이 버전은 LLM 프로젝트 시작 이래 최대 규모의 변화로, 도구(tool) 지원이 추가됨. 이제 OpenAI, Anthropic, Gemini, Ollama 등 다양한 LLM 모델을 LLM의 CLI 도구 및 Python 라이브러리와 연동하여, Python 함수로 표현 가능한 어떠한 툴에도 접근할 수 있음.

### 주요 기능 및 변화

* **플러그인에서 툴 설치 및 불러오기**: `--tool/-T` 옵션으로 설치된 플러그인을 통해 툴을 등록하고 사용할 수 있음
* **명령줄 Python 코드 전달**: `--functions` 옵션으로 직접 Python 함수 코드를 넘겨주어 툴 활용 가능
* **Python API에서도 툴 지원**: `.chain` 메서드에 `tools=[함수명]` 전달로 손쉽게 툴 연결
* **동기 및 비동기 컨텍스트 지원**: sync/async 상황 모두에서 툴 호출 가능

### 툴 사용 예시

#### 사용 준비

1. 최신 LLM 설치 또는 업그레이드 필요
2. OpenAI 등 API 키 등록:

   ```
   llm keys set openai
   # API 키 입력

   ```
3. 첫 번째 툴 실행 예시:

   ```
   llm --tool llm_version "What version?" --td

   ```

   * **llm\_version**은 기본 제공되는 간단한 데모 툴임
   * `--td`는 **툴 디버그 출력** 옵션으로, 툴 호출 과정 및 응답 내용 확인 가능
4. 모델 변경:

   ```
   llm models default gpt-4.1-mini

   ```

   * 옵션으로 다양한 모델 지원 가능 (`-m o4-mini` 등)
5. 내장 시간 툴 호출:

   ```
   llm --tool llm_time "What time is it?" --td -m o4-mini

   ```

   * 시간 및 타임존 정보를 상세히 출력함

#### 다양한 모델 및 플러그인 지원

* Anthropic Claude 4, Google Gemini 2.5 Flash, Ollama Qwen3:4b 등 **여러 LLM 모델 환경**에서 동일한 툴 인터페이스로 동작함
* 예시 명령어:

  ```
  llm install llm-anthropic -U
  llm keys set anthropic
  llm --tool llm_version "What version?" --td -m claude-4-sonnet

  ```

#### 수학 연산 및 코드 실행

* **수학에 약한 LLM 한계**를 Python 툴로 극복 가능
* [llm-tools-simpleeval](https://github.com/simonw/llm-tools-simpleeval) 플러그인을 통한 안전한 산술 연산 지원

  ```
  llm install llm-tools-simpleeval
  llm -T simpleeval
  llm -T simple_eval 'Calculate 1234 * 4346 / 32414 and square root it' --td

  ```

  + `sqrt()` 지원이 없을 경우 `** 0.5`로 대체하는 식으로, **코드 실행 유연성** 확보

#### 도구 플러그인 소개

* [llm-tools-simpleeval]: 산술 및 간단한 식 계산
* [llm-tools-quickjs]: **QuickJS** 자바스크립트 샌드박스 해석기 툴
* [llm-tools-sqlite]: 로컬 SQLite 데이터베이스 **읽기 전용 SQL 질의**
* [llm-tools-datasette]: 원격 Datasette 인스턴스에 SQL 쿼리 실행 지원

예시:

```
llm install llm-tools-datasette
llm -T 'Datasette("https://datasette.io/content";)' --td "What has the most stars?"

```

* **툴박스** 형태 플러그인이라 URL 등 인자를 통해 설정 가능
* 잘못된 컬럼 명시 시 **툴 에러 감지 및 재시도** → 스키마 조회 → 올바른 쿼리 생성 방식으로, LLM의 어댑티브 능력 입증

#### 직접 Python 코드로 툴 정의

* 명령줄에서 `--functions` 옵션으로 **임의의 Python 함수**를 전달, 즉시 툴화
* 예시:

  ```
  llm --functions '
  import httpx

  def search_blog(q):
      "Search Simon Willison blog"
      return httpx.get("https://simonwillison.net/search/";, params={"q": q}).content
  ' --td 'Three features of sqlite-utils' -s 'use Simon search'

  ```

  + 간단한 웹 검색 도구 구현 가능
  + System prompt를 통해 **모델에 툴 활용 방향성 설정**이 중요함

### Python 라이브러리로의 통합 활용

* LLM은 CLI와 Python API 모두 지원
* `.chain()` 메서드로 툴 호출 요청 감지 → 실행 → 결과 반영까지 일련 과정을 자동화
* 예시:

  ```
  import llm

  def count_char_in_text(char: str, text: str) -> int:
      "How many times does char appear in text?"
      return text.count(char)

  model = llm.get_model("gpt-4.1-mini")
  chain_response = model.chain(
      "Rs in strawberry?",
      tools=[count_char_in_text],
      after_call=print
  )
  for chunk in chain_response:
      print(chunk, end="", flush=True)

  ```
* **비동기 함수**(async def)도 지원되어 동시 실행 가능
* 툴박스 플러그인도 `tools=[Datasette(...)]` 형태로 그대로 연결

### 도구 패러다임의 발전 과정

* **툴 호출 패턴**은 2022년 ReAcT 논문 등에서 소개된 후, 모든 주요 LLM 벤더와 로컬 모델이 도입
* API마다 "Function calling" 또는 "툴 사용" 등 다양한 명칭으로 통일된 형태 채택
* Ollama, llama.cpp 등도 이미 **툴 지원** 추가 및 기능 확장

### 개발 및 운영 뒷이야기

* LLM이 오랫동안 툴 연동 지원이 필요함을 인지했으나, 다양한 모델 간 추상화 계층 설계가 어려움
* 벤더 간 툴 패턴 표준화가 진행됨에 따라 LLM 0.26에서 통합 구현이 가능해짐
* PyCon US 2025 워크숍에서 실무 활용 사례로 시연

### “에이전트”와의 관계

* "에이전트"란 용어는 아직 논란이나, 현재 LLM 생태계에서 "툴 인 더 루프" 형태로 표준화됨
* LLM 0.26은 에이전트 개발에도 적합한 구현임
* 향후 **플러그인 제작**을 위한 템플릿, 개선 이슈, 고급 플러그인 문서화 등이 활발히 진행 중

### Model Context Protocol (MCP) 지원 예정

* **MCP**는 LLM이 툴에 접근하는 새로운 표준 프로토콜로 급부상
* 지난 8일 내에 OpenAI, Anthropic, Mistral 등 대형 벤더 API에도 빠르게 도입되고 있음
* 향후 LLM을 MCP 클라이언트로 만들어 다양한 MCP 서버에 쉽게 연동 계획

### 커뮤니티 및 확장

* LLM 프로젝트 플러그인, 툴 개발 논의 및 생태계 확장을 위해 Discord 커뮤니티 운영
* 오픈소스 생태계에서 툴 기반 LLM 활용의 가능성은 **사실상 무한**함을 강조

---

이상의 방식으로 LLM 0.26은 모든 주요 LLM 모델과 연동되는, 범용적이며 확장 가능한 **툴 연동 플랫폼**으로 거듭나고 있음. 툴을 통해 언어모델의 활용 범위가 폭넓게 확장된다는 점에서, 개발자 및 IT 실무자에게 매우 의미 있는 도구임.

