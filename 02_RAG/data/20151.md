# OSINT가 AI로 인해 비판적 사고가 점진적 붕괴하고 있음


* OSINT(공개 출처 정보 분석)은 본래 ‘생각하는 게임’이었음
* 최근에는 점점 더 AI 도구에 의존하는 ‘신뢰의 게임’으로 변해가고 있음
* 문서 요약, 번역, 보고서 작성에서 시작해 AI가 주도하는 조사로 바뀌며 비판적 사고가 줄어듦
* “더 똑똑하게 일하고 있다”는 착각 속에 실제 사고 과정이 무너지고 있음

AI의 확산이 가져온 변화
--------------

* 글쓴이 역시 ChatGPT, Copilot, Claude, Gemini 등을 매일 사용 중
* 문제는 분석가들이 어려운 과정을 건너뛰고 AI에 사고를 맡기기 시작했다는 점
* OSINT는 속도만 중요한 게 아니라 판단력이 핵심인데, 판단력은 모델이 줄 수 없음
* 비판적 습관을 지키지 않으면 조사관이 아닌 자동화 운영자가 되어버림

모두가 읽어야 할 연구 결과
---------------

* 2025년 초 Carnegie Mellon과 Microsoft Research 팀이 319명의 지식 근로자를 대상으로 한 연구 발표
* 결과: AI에 대한 신뢰가 높을수록 비판적 사고는 낮아지는 경향
* 반대로 스스로에 대한 자신감이 높은 사람일수록 더 많이 질문하고 검증함
* AI에 대한 신뢰는 곧 자기 사고의 포기와 연결됨

실제 업무에서 나타난 현상
--------------

* 가설을 세우기보다는 AI에게 아이디어를 묻는 방식으로 변화
* 출처를 검증하기보다는 AI가 이미 했을 거라 생각
* 다양한 관점을 평가하기보다는 AI 요약본을 편집하고 끝냄
* 전문가들도 이런 방식에 익숙해지며 사고를 멈추는 경우가 많아짐

OSINT에서의 AI 오용 사례
-----------------

### 이미지 검증 실패

* 시위 사진을 Gemini에 업로드하고 “이 사진은 어디인가?”라고 묻자 ‘파리’라고 답변
* 표지판, 번호판, 건축 양식을 보면 벨기에가 명확하지만 AI를 믿고 넘어가 오판

### 인물 프로파일 왜곡

* Claude로 인물의 온라인 활동을 요약하면 “활동가, 기술 종사자, 무해한 인물”로 표현
* 극우 포럼 활동 내역은 누락됨 → 검증 없이 이벤트 발표자로 선정되는 위험

### 허위 정보 캠페인 분석 실패

* Telegram 메시지를 ChatGPT에 입력하여 요약과 패턴 분석 요청
* 키워드만 표시되고, 러시아 정보조작 그룹의 언어 패턴은 놓침

OSINT 분석가들이 직면한 위협
------------------

* 위 사례는 모두 매우 현실적인 OSINT 실패 가능성
* 문제는 악의적이지도, 게으르지도 않은 분석가들이 툴을 너무 신뢰했기 때문
* AI는 조사 능력을 대체할 수 없음, 무비판적 사용이 OSINT를 위험하게 함

죽어가는 OSINT 전문성 (Tradecraft)
---------------------------

* 전문성이란 ‘도구 목록’이 아닌, 의심하고 확인하는 사고 습관
* 이상한 느낌이 들 때 다시 보는 습관, 메타데이터 확인, 언어 불일치 감지 등의 본능
* AI는 일을 쉬워 보이게 만들고, 사고 과정을 없애버림
* 편리함 속에 전문성은 사라지고 있음

### 과거와 현재의 분석가 비교

**예전에는:**

* 흐릿한 이미지를 여러 도구로 분석, EXIF 정보 확인, 랜드마크로 역검색
* 외국어 게시물 수동 번역, 해시태그 추적, 계정 활동 내역 확인
* 도메인 WHOIS 분석, 서브도메인 추적, 이메일 연결 조사

**현재는:**

* 이미지를 AI에 넣고 위치만 확인하고 넘어감
* 게시물을 AI에 요약하게 하고 바로 사용
* “이 도메인을 누가 운영하나?”라고 AI에 물어보고 답을 신뢰함

전문성 상실의 결과
----------

* 문맥적 사고력, 출처 간 교차 확인, 가설 검증, 깊이 있는 탐색 능력 상실
* AI는 설득력 있는 문장과 자신감으로 착각을 유도함
* 악의적 세력은 AI의 약점을 악용해 조작된 데이터를 흘려 넣음

분석가의 새로운 역할: AI의 감시자
--------------------

* GenAI는 없어지지 않음, 문제는 그것을 ‘조력자’가 아닌 ‘판단 기준’으로 여길 때 발생
* 분석가는 이제 AI를 테스트하고, 검증하고, 의심하는 역할 수행 필요
* “답을 찾는 사람”이 아닌 “답을 깨뜨리는 사람”이 되어야 함

### 분석가 사고방식의 변화

* 예전에는 단순히 **AI에게 질문을 던지고 답을 받는** 역할이었다면,  
  이제는 **AI가 내놓은 답변을 면밀히 심문하고 검증하는** 역할로 바뀌어야 함
* 예전에는 **AI가 요약해 준 내용을 그대로 받아들이는** 데 그쳤다면,  
  이제는 **그 요약을 세세히 뜯어보고 어떤 정보가 빠졌고 어떤 해석이 들어갔는지를 분석해야** 함
* 예전에는 **AI가 준 제안을 그대로 쓰거나 따르는** 경우가 많았다면,  
  이제는 **그 제안이 왜 그렇게 나왔는지 분해하고 다시 구성해보는 작업**이 필요함
* 예전에는 **AI가 제시한 깔끔하고 단정적인 답을 믿는** 경향이 있었다면,  
  이제는 **그 답이 어디서 왔는지, 어떤 출처를 기반으로 했는지를 더럽고 복잡하더라도 추적해야 함**
* 예전에는 **어떤 인물이나 사건에 대한 프로파일을 AI에게 맡겨 작성**했다면,  
  이제는 **그 프로파일 속 내러티브가 실제 맥락과 맞는지를 스스로 검증하는 작업**이 중요함
* 예전에는 **AI가 초안만 잘 만들어 주면 그대로 제출**했지만,  
  이제는 **그 초안을 해체하고, 문제점을 찾아내고, 다시 조립하여 진짜 내 것으로 만드는 과정**이 필요함

비판적 사고를 되살리는 방법
---------------

### 일부러 ‘마찰’을 추가하기

* 너무 빠른 결과는 위험
* AI가 제공한 정보도, 내가 원래 하던 검증 절차를 그대로 실행

전술:

* “AI가 없었다면 뭘 했을까?” → 그 작업을 실제로 실행
* AI 출력이 맞는지, 반례를 일부러 찾음
* 다른 모델에 “정반대의 해석을 해봐”라고 요청

### 출처 확인 습관 복원

* GenAI는 OSINT식 인용을 하지 않음
* 모델이 제공한 이름, 링크, 인용구는 반드시 역추적 필요

전술:

* AI 결과 vs. 실제 출처를 나란히 비교
* 요약을 보더라도 원문은 반드시 열어볼 것

### AI를 ‘생각 파트너’로 취급

* AI는 주니어 분석가일 뿐, 감독이 필요함

전술:

* 내 가설에 반박하도록 요청
* 나의 조사 노트를 제공하고 빠진 내용 지적 요청
* 다양한 관점을 시뮬레이션하게 활용

### 모델 간 비교

* ChatGPT, Claude, Gemini, Copilot의 출력을 비교
* 차이점을 신호로 간주하고 원인 탐색

### 일부러 모델을 ‘깨뜨리기’

* 의도적으로 모순되거나 애매한 질문을 던짐
* 오류 발생 패턴 파악 → 인간의 판단력으로 보완

### ‘어려운 작업’을 계속 수행하기

* AI는 보조 도구, 핵심 작업은 직접 해야 함

전술:

* AI를 쓰기 전 직접 지오로케이션 시도
* AI 요약을 보기 전 직접 요약 작성
* AI로 프로파일 작성 전, 내가 직접 프로파일링 후 비교

조용한 붕괴와 그에 맞서는 방법
-----------------

* 비판적 사고의 붕괴는 갑자기 오지 않음
* 보고서가 더 빠르고 더 깔끔해질수록 위기일 수 있음
* 정답처럼 보이는 정보, 확인 없이 믿는 습관이 위험

그러나, 이 모든 건 되돌릴 수 있음

* AI를 배제할 필요는 없음
* 대신, AI와 맞서고, 의심하고, 반박해야 함
* 당신은 ‘도구 사용자’가 아니라 ‘조사자’임

✅ OSINT AI 남용 방지 체크리스트
----------------------

* ✅ AI 출력의 원본을 추적했는가?
* ✅ AI 결과를 수용하기 전에 비-AI 출처를 참고했는가?
* ✅ 반대 가설이나 다른 모델로 도전했는가?
* ✅ 두 개 이상의 인간 소스를 통해 교차 확인했는가?
* ✅ 최소 한 작업을 수동으로 실행했는가?
* ✅ AI 출력에 암묵적 가정이 있는지 확인했는가?
* ✅ AI를 진실의 출처가 아니라 사고 파트너로 취급했는가?
* ✅ 의도적으로 검증 과정을 느리게 만들었는가?
* ✅ “내가 확인 없이 믿고 있는 건 무엇인가?”라고 자문했는가?
* ✅ 우리 OSINT 결과물에 AI 사용 여부를 독자에게 명시했는가?
