# Google Cloud 장애 보고서 – 2025-06-13


* 2025년 6월 12일, **Google Cloud**와 **Google Workspace** 서비스에서 외부 API 요청 중 **503 오류**가 전 세계적으로 증가함
* 오류 원인은 **Service Control** 시스템의 코드 변경과 정책 데이터에 빈 필드가 포함된 잘못된 정책 반영임
* 핵심 바이너리의 **에러 처리 미흡**과 기능 플래그 미적용 등이 문제 확산을 키웠음
* 복구는 2~3시간이 소요되었으며, us-central-1 지역은 인프라 과부하로 더 긴 복구 시간 발생함
* Google은 **아키텍처 분리, 에러 처리 개선, 데이터 검증 강화** 등 재발 방지 대책을 발표함

---

전체 장애 개요
--------

### Google Cloud 및 Google Workspace 서비스 장애 요약

* 2025년 6월 12일 오전 10시 49분(PDT)부터, Google Cloud, Google Workspace, Google Security Operations를 포함하는 여러 서비스에서 **외부 API 요청**에 대해 **503 오류**가 급증하는 현상 발생함
* 고객 서비스와 신뢰에 심각한 영향을 주었음에 대해 Google 측은 깊은 사과 의사를 밝힘
* Google API 관리 및 제어 플레인은 각 요청의 **정책 및 쿼터 체크**를 담당하며, 핵심 체크 시스템은 ‘Service Control’이라는 바이너리로 동작함

장애 원인 분석
--------

### 변화된 시스템 구조 – Service Control

* 2025년 5월 29일, Service Control에 **쿼터 정책 검사를 강화하는 신규 기능**이 추가됨
* 지역별로 단계적 출시를 진행했으나, 문제의 코드는 **정책이 실제로 반영되었을 때만 동작**하며, 기존에는 트리거되지 않아 사전 테스트가 미흡했음
* 해당 신기능 경로에 적절한 **에러 처리와 기능 플래그**가 부재하여, null 포인터 상황에서 바이너리가 연쇄적으로 크래시됨

### 장애 발생 경위

* 2025년 6월 12일 오전 10시 45분(PDT), 정책 변경이 Regional Spanner 테이블에 삽입됨
* 이 정책 데이터에는 의도하지 않은 **빈 필드(Blank Field)** 가 포함되어 있었으며, 이것이 전 세계적으로 거의 실시간 복제됨
* Service Control이 이 정책을 처리하면서 **null 포인터**에 의한 크래시가 발생, 각 지역 인스턴스가 전역적으로 Crash Loop에 빠짐
* 2분 만에 SRE팀이 인지를 시작했고, 10분 내에 원인을 파악 후 임시로 바이너리 경로를 차단(red-button), 40분 만에 대부분의 지역은 복구됨

### 추가 복구 이슈

* 일부 대형 지역(us-central-1)은 Service Control 태스크 재시작 시 **herd effect**로 인프라(Spanner 테이블)가 과부하됨
* Service Control이 **무작위 지수적 백오프**를 적용하지 않아 인프라 부담 가중됨
* 해당 지역은 2시간 40분까지 복구 지연, 트래픽 우회 등으로 영향 최소화했으며, 전체적으로 서비스 복구 완료됨

고객 영향 및 장애 범위
-------------

* 고객은 **API 및 사용자 인터페이스 접속 장애** 발생, 스트리밍 및 IaaS 리소스에는 영향 없음
* 지연 및 백로그 영향은 최대 1시간 이상 일부 서비스에서 지속
* 장애 영향을 받은 Google Cloud와 Google Workspace 제품 리스트가 광범위하게 제시됨
  + 예: IAM, Cloud Build, Cloud Storage, BigQuery, AppSheet, Gmail, Google Drive 등 수십여 개 서비스

향후 개선 방안
--------

* **서비스 아키텍처를 모듈화**하여 각 기능 분리 및 장애 발생시 개방형(fail open) 처리 도입
* **글로벌 데이터 복제 단계적 전파** 및 실질적인 검증 과정 강화
* 모든 주요 바이너리 변경 시 **기능 플래그화 및 기본 비활성 처리** 적용 정책 개편
* **정적 분석과 테스트 개선**을 통해 에러 감지 및 장애 시 fail open 가능하게 설계 검토
* **무작위 지수적 백오프** 정책 및 모니터링/커뮤니케이션 신뢰도 강화 예정
* 장애 상황에서도 고객에게 신속하게 **모니터링 및 정보 전달**이 가능하도록 인프라 이중화와 자동화 커뮤니케이션 보완

장애 공지 및 커뮤니케이션
--------------

* 사고 후 1시간 이내에 Cloud Service Health에 공지하였으나, 모니터링 인프라 자체도 장애 발생
* 일부 고객은 Google Cloud 기반의 모니터링 시스템 자체가 정상 작동하지 않아 장애 신호 및 영향 파악 곤란함
* Google은 향후 **모니터링 및 대고객 커뮤니케이션 인프라 강화**를 약속함

주요 장애 타임라인 (미니 리포트 요약)
----------------------

* **장애 시작:** 2025년 6월 12일 10:49 (PDT)
* **대부분 지역 복구:** 2025년 6월 12일 12:48 (PDT)
* **장애 종료:** 2025년 6월 12일 13:49 (PDT)
* **총 소요:** 약 3시간
* **영향 지역:** 전세계

사후 대책 요약
--------

* **API 관리 플랫폼**의 데이터 오류나 손상 시 실패 방지 장치 마련 예정
* **글로벌 메타데이터 전파**전 검증·테스트·모니터링 강화
* 유효하지 않은 데이터에 대한 **시스템 에러 처리 및 종합 테스트** 확대

영향 서비스 리스트 (발췌)
---------------

### Google Cloud 주요 서비스

* Identity and Access Management, Cloud Build, Google Cloud Storage, Cloud Monitoring, BigQuery, Vertex Gemini API, Cloud Firestore, Looker, Cloud Run, Compute Engine 등

### Google Workspace 주요 서비스

* AppSheet, Gmail, Google Drive, Google Meet, Docs, Chat, Calendar 등

결론
--

* 이번 장애는 **정책/쿼터 관리 시스템 구조**, **데이터 무결성 검증 부족**, **에러 처리 체계 부재**가 복합적으로 작용한 문제임
* Google은 **아키텍처 레벨에서의 개선** 및 장애 대응력 강화를 약속함
