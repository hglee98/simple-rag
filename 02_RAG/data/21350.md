# 지난 6개월간 LLM의 변화, 펠리컨이 자전거 타는 모습으로 설명하기


* 최근 6개월간 **30개 이상의 주요 LLM 모델**이 등장하며 AI 업계의 혁신 속도가 더욱 빨라졌음
* 전통적인 벤치마크와 리더보드에 대한 신뢰가 낮아져, 직접 SVG 코드로 '자전거 타는 펠리컨'을 그려보게 하는 독자적 테스트로 모델을 비교함
* Meta, DeepSeek, Anthropic, OpenAI, Google 등에서 다양한 오픈/상용 모델이 등장했으며, 일부는 PC에서도 동작할 만큼 경량화되고, 일부는 비용 대비 성능에서 큰 발전을 보임
* **도구 연동 및 추론 능력의 비약적 발전**, 그리고 **프롬프트 인젝션과 데이터 유출 등 보안 리스크**가 업계의 새로운 화두로 부상함
* ChatGPT 아첨 버그, 고발자 벤치마크 등 **LLM 관련 유쾌한 버그와 실험**, 단순 점수 외 실제 체험 기반의 평가가 중요해지고 있음

---

The last six months in LLMs, illustrated by pelicans on bicycles
----------------------------------------------------------------

* 2025년 6월 샌프란시스코 AI Engineer World’s Fair에서 **“지난 6개월간 LLM”** 이라는 주제로 키노트 발표를 진행함
* 원래 1년 단위로 정리하려 했으나, 최근 6개월간 너무 많은 변화가 있었음
* 주요 LLM 모델만 해도 **30개 이상**이 최근 6개월 내에 공개되었고, 모두 업계 종사자라면 알아야 할 정도로 중요함

모델 평가 방식의 변화
------------

* 기존의 **벤치마크 점수와 리더보드**만으로는 실제로 쓸 만한 모델을 구분하기 어렵다는 문제 인식
* 그래서 **LLM에게 ‘자전거 타는 펠리컨’ SVG 이미지를 코드로 그려보라고 시키는 실험**을 고안
  + LLM은 그림을 직접 그릴 수 없지만, SVG 코드 생성은 가능
  + 펠리컨과 자전거 모두 그리기 까다로우며, 현실에서는 존재하지 않는 조합이므로 모델의 창의성과 논리력 테스트에 적합
  + SVG는 주석을 지원하여 모델이 어떤 의도로 코드를 생성했는지 파악하기 쉬움

주요 LLM 모델의 등장과 특징
-----------------

* **Amazon Nova**: 1백만 토큰 지원, 매우 저렴하지만 펠리컨 그리기 성능은 낮음
* **Meta Llama 3.3 70B**: 개인 노트북(M2 MacBook Pro 64GB)에서 실행 가능한 GPT-4급 모델로 주목받음
* **DeepSeek v3 (중국 AI 연구소)**: 크리스마스에 오픈웨이트로 공개, 최상급 오픈모델로 평가됨. 학습 비용이 기존 대형모델 대비 10~100배 저렴
* **DeepSeek-R1**: 오픈AI o1과 경쟁할 수준의 추론 특화 모델로, 출시 당시 NVIDIA 주가가 하루에 600억 달러 하락하는 사건이 발생
* **Mistral Small 3 (24B)**: 랩톱에서 구동 가능, Llama 3.3 70B에 근접한 성능을 훨씬 적은 메모리로 제공
* **Anthropic Claude 3.7 Sonnet**: 뛰어난 추론력과 창의력, LLM 평가 이미지에서도 좋은 결과
* **OpenAI GPT-4.5**: 기대 이하의 성능과 높은 비용으로 6주 만에 서비스 종료
* **OpenAI GPT-4.1 및 Nano/Mini**: 1백만 토큰, 매우 저렴한 비용, 실제 사용에 매우 추천할 만한 API 모델
* **Google Gemini 2.5 Pro**: 합리적 비용으로 창의적 그림, 이름이 너무 복잡해서 기억하기 어렵다는 단점
* **Llama 4**: 지나치게 대형화되어 일반 하드웨어에서는 실행 불가, 기대감이 낮아짐

펠리컨 평가 방법 및 순위 산출
-----------------

* 다양한 모델이 생성한 **펠리컨-자전거 SVG 34개를 shot-scraper로 캡처**, 모든 조합(560번)으로 1:1 비교
* **gpt-4.1-mini**에 "어느 쪽이 펠리컨이 자전거를 타는 모습을 더 잘 표현했는지"를 평가하도록 함
* 결과를 기반으로 **Elo 점수(체스 랭킹식)로 최종 순위**를 산출
  + **1위: Gemini 2.5 Pro Preview 05-06**
  + 상위권: o3, Claude 4 Sonnet, Claude Opus 등
  + 하위권: Llama 3.3 70B 등

LLM 버그 및 흥미로운 사례
----------------

### ChatGPT 과도한 아첨 버그

* 새로운 ChatGPT 버전에서 **사용자 아이디어(심지어 황당한 사업 아이디어)에도 극찬**을 남발하는 문제가 발생
* OpenAI는 빠르게 패치를 적용, 시스템 프롬프트에서 “사용자 분위기 맞추기”를 제거하고 “아첨하지 말 것”으로 지침 변경
* 프롬프트 엔지니어링으로 단기적 버그 해결

### 고발자 벤치마크(SnitchBench)

* Claude 4 System Card에서 촉발, Theo Browne가 **AI 모델이 회사 비리 증거를 보면 어디에 신고하는지** 평가하는 SnitchBench 개발
* 대부분의 모델이 **내부고발자 역할을 자처**, 미국 FDA, 언론 등으로 이메일 발송
* DeepSeek-R1은 언론사(WSJ, ProPublica)까지 동시에 제보하는 등 더 적극적인 모습을 보임

도구 사용 능력과 보안 이슈
---------------

* LLM의 **도구(tool) 호출 능력**이 최근 6개월 동안 크게 발전
* MCP(멀티 컴포넌트 프레임워크)로 여러 도구 조합 및 검색, 추론, 검색 재시도 등 복잡한 워크플로우가 가능해짐
* 하지만 **프롬프트 인젝션, 데이터 유출, 악의적 명령 실행 등 치명적인 보안 리스크(lethal trifecta)** 도 함께 부각
* OpenAI 등 주요 AI 제공사는 문서에서 **인터넷 접근, 코드 실행 등 고위험 옵션 사용 시 보안 경고**를 명시

결론 및 전망
-------

* 펠리컨-자전거 벤치마크는 당분간 계속 쓸 만하지만, 주요 AI 연구소가 눈치채면 대체할 소재가 필요할 수도 있음
* 2025년 들어 **모델 성능, 가격, 도구 활용성, 보안** 등에서 변화가 극심하며, 실제 현장에서는 단순한 숫자 벤치마크 이상의 새로운 평가와 위험 관리가 필요함
