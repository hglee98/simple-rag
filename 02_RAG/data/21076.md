# 게임을 넘어 현실까지 배우는 AI: 존 카맥의 현실 기반 강화학습 도전


* John Carmack의 "Upper Bound 2025 발표"의 준비노트 요약 및 슬라이드
* **존 카맥**은 Id Software, Oculus, Keen Technologies 등을 거친 후 현재는 **강화학습 기반 AGI 연구에 집중**하고 있음
* **LLM을 지양하고**, 동물처럼 환경과 상호작용하며 배우는 **지속적·효율적 학습**에 관심을 둠
* **고전 게임 Atari**를 기반으로 **실시간 카메라·조이스틱 입력**으로 학습하는 **물리적 RL 시스템을 구축**함
* **속도·지연·연속학습·망각 방지** 등 RL 시스템이 현실과 유사해지기 위해 해결해야 할 기술적 과제를 폭넓게 제시함
* **CNN 구조, 보상 표현, 탐험 전략** 등에 대해 경험 기반의 날카로운 통찰을 공유하며, **기존 관행에 의문을 제기**함

* 슬라이드: [https://docs.google.com/presentation/d/…](https://docs.google.com/presentation/d/1GmGe9ref1nxEX_ekDuJXhildpWGhLEYBMeXCclVECek/edit)
* 준비 노트: [https://docs.google.com/document/d/…](https://docs.google.com/document/d/1-Fqc6R6FdngRlxe9gi49PRvU97R83O7ZTN_KFXo_jf0/edit?usp=drivesdk)

---

Quick Background
----------------

* **Id Software 창업자**로서 Quake는 GPU 발전을 이끌며 AI 분야에 간접적 영향을 줌
* Armadillo Aerospace에서 **수직이착륙 로켓 연구**를 10년간 수행
* Oculus에서 **현대 VR 기술의 토대 구축**
* Keen Technologies 설립, **강화학습에 집중**하며 AI 연구에 전념 중
* **리처드 서튼**과 함께 연구 중으로, **강화학습에 대한 철학을 공유**함

Where I thought I was going
---------------------------

### Not LLMs

* **LLM은 “학습 없는 지식”** 으로, 본인이 지향하는 상호작용 기반 학습과는 철학이 다름
* LLM이 RL을 대체할 가능성은 열려 있으나, **동물처럼 환경에서 배우는 방식에 더 매력을 느낌**

### Games

* 오랜 게임 개발 경력 덕분에 **게임을 실험 환경으로 활용**
* DeepMind의 Atari 연구처럼 **픽셀 기반 입력만으로 학습 가능성**을 타진
* 그러나 **막대한 학습 프레임 수**와 **효율성 문제**는 여전히 과제
* **다중 과제, 온라인, 효율적 학습은 미해결 상태**

### Video

* 원래는 TV 같은 **수동적 영상 학습을 고려**, 그러나 **게임 학습 자체에 집중**하기로 함

Missteps
--------

* **너무 로우레벨(C++ CUDA)에서 시작**, PyTorch로 전환하며 실험 속도 향상
* Atari 대신 **Sega Master System으로 시작했으나 비교 자료 부족으로 전환**
* **비디오 기반 학습은 보류**, 게임 내 학습만으로도 충분한 과제가 있음

Settling in with Atari
----------------------

* **상업용 게임의 다양성**은 연구 편향을 줄여주는 장점
* **ALE 직접 사용** 권장 (Gym 등 래퍼는 문제 발생 가능성 있음)
* 최신 모델이 대부분의 게임을 고득점으로 해결했지만, **“Atari 100k”처럼 데이터 효율성 있는 학습이 더 중요**
* **환경의 결정론적 행동**은 Sticky action 도입 등으로 극복 필요

Reality is not a turn based game
--------------------------------

* 현실은 에이전트를 기다려주지 않음 → **비동기 처리와 지연 고려 필요**
* **단일 환경에서의 학습 실패는 알고리즘 자체 문제를 시사**
* **속도**: 고속으로 평가 가능한 정책이 필요 (CUDA graph 활용 등)
* **지연**: RL 알고리즘 대부분은 지연에 취약함 → 정책 적용 지연을 반영하는 구조가 필요

Physical Atari
--------------

* **물리 환경에서의 Atari 학습 시스템 구축**
* 실제 조이스틱 조작, 화면을 보는 카메라, RL 에이전트가 실시간으로 작동
* 여러 게임을 테스트하며 **점수 인식·행동 지연·조작 오류 등 현실 문제 고려**
* **조이스틱 동작은 불안정**, 점수 인식이 가장 까다로움
* 일부 게임은 점수가 잘 보이지 않아 제외함

Sparse rewards / Curiosity
--------------------------

* RL은 보상이 희소한 환경에 약함 → **내재적 보상, 인공지능적 호기심 활용**
* **게임 점수 자체를 보상 대신 사용할 수 있는가**에 대한 탐색도 병행
* **게임 간 전환**, **새로운 게임에 대한 흥미 유지** 같은 인간 행동 패턴 재현 시도

Sequential multi-task learning
------------------------------

* **연속 학습 환경에서의 망각 문제** (catastrophic forgetting)은 여전히 심각
* 사람은 오래된 기술을 기억하는데, **현재의 모델은 과거 게임 재방문 시 성능 급락**
* **기억 보존, 학습률 조정, 가중치 sparsity 등으로 개선 시도**
* **Task ID 사용은 부정행위**로 간주, implicit하게 전환 필요

Transfer Learning
-----------------

* 학습이 많은 게임을 통해 더 빠르게 새 게임을 배워야 함
* **OpenAI의 Sonic 챌린지는 결국 다시 from scratch 학습**
* GATO 등은 **부정적 전이(negative transfer)** 발생
* **“천천히 배워야 빨리 배운다”는 전략**이 필요할 수 있음
* **새로운 벤치마크 제안**: 여러 게임을 순차적으로 반복하면서 점수 평가

Plasticity vs generalization
----------------------------

* **일반화**는 무시하는 것이고, **가소성**은 새 패턴 인식 → 서로 충돌할 수 있음
* 일반화는 약한 이론 기반, CNN의 inductive bias 정도
* **강화학습의 값 함수는 일반화의 산물이며, 매우 민감**

Exploration
-----------

* **랜덤 액션 선택의 한계** → 실수 하나로 생존 좌우됨
* **액션 공간 구조화**, confidence 기반 정책 등 시도
* **시간 단위 액션** 역시 고민 필요 → 60fps 학습은 어려움이 큼

Recurrence vs frame stacks
--------------------------

* Atari에서는 frame stack이 효과적이지만, **recurrent 구조는 뇌와 더 유사**
* Transformer는 batch 학습에는 강하지만 **일반 recurrent online 학습은 미완**

Function approximation 중심의 학습
-----------------------------

* NN은 값 추정, 일반화, 확률 평균, 정책 개선을 동시에 수행
* 모든 가중치 업데이트는 **모든 출력값에 영향**
* **초기화·활성함수·옵티마이저 조합**이 성능에 중대한 영향

Value representation
--------------------

* **클래식 DQN reward clamping**은 학습 안정화에 유효
* **Categorical 표현, MSE 활용, MuZero의 value 압축 등 다양한 접근** 존재
* **게임마다 점수 범위가 달라 multi-task 학습에서 문제**

Conv Nets
---------

* **CNN은 여전히 RL의 기본 구조**
* 대형 이미지 네트워크는 RL에서 성능 하락 (예: ConvNeXT)
* **커널 구조 변경, 파라미터 공유, Isotropic CNN 등 실험**
* DenseNet, Dilated CNN 등 **효율적인 정보 흐름** 추구
* **생물학적 구조와 유사한 CNN 개선 시도**
