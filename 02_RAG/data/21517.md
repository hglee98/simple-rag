# Google, Gemini 2.5 Flash/Pro 정식 출시 및 `Flash-Lite` 모델 공개


* **Gemini 2.5 Flash와 Pro 모델의 정식 출시**와 함께, **가장 저렴하고 빠른 Flash-Lite 모델의 프리뷰 버전**을 공개함
* Flash-Lite는 **번역, 분류 등 지연에 민감한 작업**에 특화되어 있으며, **2.0 Flash/Flash-Lite보다 낮은 지연 시간**과 **높은 전반적 품질**을 제공함
* 모든 2.5 모델은 **멀티모달 입력**, **1M 토큰 컨텍스트 길이**, **도구 연결(검색, 코드 실행 등)**, **Thinking 모드 전환 가능** 등의 기능을 지원
* **비용 대비 성능 최적화(Pareto Frontier)** 를 고려한 설계로, 대규모 트래픽 처리에 적합한 제품군 구성을 갖춤
* Flash-Lite 및 Flash는 **검색에도 커스터마이징되어 활용 중**, 개발자는 **Google AI Studio와 Vertex AI에서** 프리뷰 혹은 정식 모델 사용 가능함

Flash-Lite의 특징
--------------

* **가장 저렴하고 빠른 모델**로, 입력 100만 토큰당 $0.10, 출력 100만 토큰당 $0.40의 요금으로 제공됨
* **비용 대비 성능이 우수**하여 번역, 분류 등 대량의 요청이 들어오는 작업에 특히 적합함
* 이전 2.0 Flash-Lite보다 전반적으로 품질이 향상되었으며, **과학(GPQA)** 기준으로 64.6% → 66.7%, **수학(AIME 2025)** 에서는 49.8% → 63.1%로 향상됨
* **코드 생성과 편집**에서는 각각 34.3%, 27.1% 수준으로, 고성능 모델 대비 낮지만 비용 대비 효율적인 선택지임
* **멀티모달 처리** 성능은 72.9%로 유지되며, **이미지 이해**는 51.3%에서 57.5%로 개선됨
* **추론(Thinking) 모드**를 활성화하면 전반적인 정확도가 상승하며, 예를 들어 **HumanEval**에서는 5.1% → 6.9%, **SWE-bench multi-task**에서는 42.6% → 44.9%로 증가함
* **사실성(SimpleQA)**, **긴 문맥 이해(MRCR)** 등에서도 Thinking 모드에서 성능이 눈에 띄게 향상되며, 특히 1M 토큰 기준 긴 문맥 정확도는 5.4%에서 16.8%로 3배 이상 향상됨
* **다국어 능력(MMLU)** 역시 높아져 Non-thinking에서는 81.1%, Thinking에서는 84.5%까지 도달함

* Gemini 2.5 모델 패밀리에 대한 **기술적 세부 내용**은 [Gemini technical report](https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_report.pdf)에서 확인 가능
