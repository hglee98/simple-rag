# Qwen3를 MacBook에서 실행하여 무료 Vibe 코딩하기


* **MLX 라이브러리**를 활용해 Mac에서 직접 실행하여, 강력한 최신 Qwen3-30B-A3B-8bit 모델을 **로컬에서 서빙**
* Localforge에서 이를 OpenAI API 방식으로 연동하여 **에이전트 루프**를 구성
* **ollama 기반 보조 모델(Gemma3)을 추가 구성**하여 에이전트 보조 역할을 분리하여 효율적 도구 사용이 가능
* 에이전트는 Localforge의 UI에서 설정 후 "LS 툴 실행", 웹사이트 생성, 스네이크 게임 자동 실행까지 수행
* 이 모든 과정은 **무료이며 로컬에서 완전 자율적으로 작동 가능**, Mac 유저라면 직접 시도해볼 만한 프로젝트

---

Qwen3를 Mac에서 로컬 실행하기
--------------------

* **목표:** 최신 Qwen3 모델을 Mac에서 실행하고 Localforge로 에이전트화하여 코딩 자동화를 실험
* Qwen3는 Ollama 및 HuggingFace MLX 커뮤니티에 배포됨
  + [Qwen3 on Ollama](https://ollama.com/library/qwen3)
  + [Qwen3 on Huggingface MLX](https://huggingface.co/collections/mlx-community/qwen3-680ff3bcb446bdba2c45c7c4)
* 1단계: MLX 환경 설치
  --------------

  ```
  pip install mlx  
  pip install mlx-lm  

  ```
* 2단계: 모델 서버 실행
  -------------

  ```
  mlx_lm.server --model mlx-community/Qwen3-30B-A3B-8bit --trust-remote-code --port 8082  

  ```

  + 모델을 자동 다운로드 후 8082 포트에서 API 서버로 구동
  + 로그에 "Starting httpd..." 메시지가 나오면 정상 실행됨

Localforge 설정
-------------

* 공식 사이트: [<https://localforge.dev>](https://localforge.dev)
* 설치 후 설정에서 다음 구성 필요:
* 프로바이더 추가
  --------

  + a) Ollama 프로바이더 (보조 모델)
    - 이름: **LocalOllama**
    - 타입: **ollama**
    - 설치 필요: gemma3:latest 모델 (단순한 언어 처리에 적합)
  + b) Qwen3 프로바이더 (주 모델)
    ---------------------

    - 이름: **qwen3:mlx:30b**
    - 타입: **openai**
    - API 키: `"not-needed"`
    - API URL: `http://127.0.0.1:8082/v1/`
* 에이전트 생성
  -------

  + 이름: **qwen3-agent**
  + 메인 모델: **qwen3:mlx:30b** (모델명: `mlx-community/Qwen3-30B-A3B-8bit`)
  + 보조 모델: **LocalOllama** (모델명: `gemma3:latest`)

결론
--

* Mac에서 **무료로 대형 모델을 로컬에서 실행해 에이전트 기반 자동 코딩이 가능**
* 모델 선택이나 시스템 프롬프트 튜닝을 통해 더 정교한 결과도 가능
* **Localforge + MLX + Qwen3**는 **개인용 LLM 실험에 매우 유용**한 조합임
