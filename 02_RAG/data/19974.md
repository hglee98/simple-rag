# 오픈소스 개발자, AI 크롤러가 트래픽을 초래하면서 국가 전체를 차단


* AI 크롤러가 오픈소스 프로젝트 사이트에 과도한 트래픽을 유발하면서 실제로 서비스 마비 수준의 피해 발생
* AI 크롤러는 `robots.txt` 무시, User-Agent 위조, 거주지 IP 우회 등으로 기존 방어 체계를 회피함
* 개발자 Xe Iaso는 이를 막기 위해 VPN 뒤로 서버를 옮기고, 사용자가 퍼즐을 풀어야 접속할 수 있는 '**Anubis**'라는 증명 기반 시스템 도입
* LibreNews에 따르면 어떤 프로젝트의 경우 전체 트래픽의 97%가 AI 크롤러에서 유입됨
* Fedora, GNOME, KDE 등의 유명 프로젝트도 국가 차단, Anubis 적용, 임시 셧다운 등으로 대응 중

실제 피해 사례와 AI 크롤러의 무분별한 접근
-------------------------

* GNOME의 GitLab에서 84,056건 중 3.2%만이 Anubis 통과 → 대부분이 비정상 크롤링으로 추정됨
* KDE는 Alibaba IP로부터의 트래픽으로 GitLab 인프라가 일시적으로 마비됨
* 모바일 사용자 일부는 퍼즐 로딩에 2분 이상 소요되기도 함
* Diaspora 인프라 유지 담당 Dennis Schubert는 AI 크롤러 트래픽을 "인터넷 전체에 대한 DDoS"로 표현함
* Read the Docs는 AI 크롤러 차단 후 하루 800GB → 200GB로 트래픽 감소, 매월 약 $1,500 절감 효과 발생

오픈소스 프로젝트에 집중된 불균형한 부담
----------------------

* 오픈소스는 제한된 자원으로 운영되며, 공개 협업을 기반으로 함
* 많은 크롤러들이 `robots.txt`를 무시하고, User-Agent를 속이며, IP를 계속 바꾸며 접근함
* Inkscape의 Martin Owens는 브라우저 정보를 위조하는 AI 회사들로 인해 대규모 차단 목록 유지 중
* Hacker News에서는 AI 기업의 자본력과 비협조적인 태도에 대한 분노 확산
* SourceHut의 Drew DeVault는 크롤러들이 모든 git 로그 페이지, 커밋까지 접근하여 리소스 과소비 유발
* Curl 프로젝트는 AI가 생성한 허위 버그 리포트를 수령한 사례 보고됨

AI 크롤러의 목적과 기업별 행동 양상
---------------------

* AI 크롤러는 학습 데이터 수집 또는 AI 답변을 위한 실시간 검색 등 다양한 목적 존재
* Diaspora 분석 결과: OpenAI 25%, Amazon 15%, Anthropic 4.3% 트래픽 차지
* 크롤러는 주기적으로 동일 페이지 반복 크롤링(예: 6시간 간격)
* OpenAI, Anthropic 등은 비교적 정상적인 User-Agent 사용, 일부 중국 AI 기업은 위장 수준 높음
* Amazon, Alibaba 등도 피해 사례에 등장하지만 해당 기업은 아직 공식 입장 없음

대응 수단: Tarpit, 퍼즐, 협업 방안 등
--------------------------

* "Nepenthes"라는 툴은 AI 크롤러를 끝없는 가짜 콘텐츠 미로에 빠뜨리는 공격적 방어 수단
* 제작자 Aaron은 이 툴이 크롤러 비용을 증가시키고 훈련 데이터 오염을 유도한다고 주장
* Cloudflare는 상업용 보안 기능으로 'AI Labyrinth'를 발표, 크롤러를 유도해 무의미한 페이지 탐색 유도
* 하루 500억 개 이상 AI 크롤링 요청이 Cloudflare 네트워크에 발생
* 오픈소스 프로젝트 "ai.robots.txt"는 AI 크롤러 목록 및 차단용 robots.txt / .htaccess 파일 제공

지속되는 AI 데이터 수집과 오픈웹의 위기
-----------------------

* 규제 없이 방대한 데이터 수집을 계속하는 AI 기업들로 인해 오픈소스 인프라에 심각한 위협 발생
* AI가 의존하는 디지털 생태계를 스스로 파괴하고 있다는 비판 제기
* 협업적인 데이터 수집 체계가 대안이 될 수 있지만, 주요 AI 기업들은 자발적 협력 의지 부족
* 의미 있는 규제나 자율적인 책임 의식 없이는 AI와 오픈소스 간의 충돌은 더욱 심화될 가능성 있음
