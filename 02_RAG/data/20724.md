# LLM 샘플링의 모든 것: 더미를 위한 현대적 가이드


* **대형 언어 모델(LLM)의 샘플링 방식**을 초보자도 이해할 수 있도록 설명한 종합 가이드
* **토큰이란 무엇인지, 왜 단어 대신 사용하는지**, 그리고 **모델이 텍스트를 생성하는 과정**이 상세하게 정리
* 샘플링은 출력의 다양성과 자연스러움을 조절하는 과정이며, **Temperature, Top-K, Top-P, DRY** 같은 **다양한 샘플링 알고리즘**을 소개
* 각 샘플링 기법은 **개념적 설명과 함께 수학적·알고리즘적 동작 원리**를 포함해 설명되며, **반복 방지**, **창의성 향상**, **일관성 조정** 등의 효과를 비교
* 샘플러 간 **조합 순서, 상호작용, 충돌 사례**까지 정리되어 있어, **생성 품질을 정교하게 제어하고 싶은 개발자에게 매우 유용한 자료**

---

Intro Knowledge
---------------

### Short Glossary

* **Logits**는 각 토큰의 점수를 나타내는 미정규화된 값임
* **Softmax**는 logits를 정규화된 확률 분포로 변환하는 함수임
* **Entropy**는 예측 불확실성을 의미하며, 높을수록 다음 토큰에 대한 불확실성이 큼
* **Perplexity**는 낮을수록 모델의 확신이 높다는 것을 의미하는 지표임
* **n-gram**은 연속된 n개의 토큰 시퀀스를 의미함
* **Context window**는 모델이 한 번에 처리할 수 있는 최대 토큰 수임

### Why tokens?

#### Why not letters?

* 글자 단위 토큰화는 시퀀스가 너무 길어져 **연산 비용 증가 및 정보 연결 어려움**이 발생함

#### Why not whole words?

* 단어 기반은 **어휘 크기 폭증**, **신조어나 희귀 단어 표현 어려움** 등의 문제가 있음
* **Sub-word** 기반은 접두사, 어근, 접미사를 나눠 처리할 수 있어 **형태소 이해와 다국어 전이 학습에 효과적**임

#### How are the sub-words chosen?

* **학습 데이터의 대표 샘플**을 통해 가장 빈번한 **부분 단어(sub-word)** 들을 찾는 방식으로 사전 구축함

### How does the model generate text?

* 학습 중에는 대량의 텍스트를 통해 다음 토큰의 **확률 분포**를 학습함
* 추론 시에는 모든 가능한 토큰에 대해 확률을 계산하고, **샘플링 기법에 따라 다음 토큰을 선택함**

### From Tokens to Text

* **예측 단계**: 모든 후보 토큰에 대한 확률 분포 계산
* **선택 단계**: 특정 샘플링 전략에 따라 토큰을 선택
* 단순히 가장 확률 높은 토큰을 고르는 것만으로는 **지루하거나 반복적인 텍스트가 생성**되므로 **샘플링 기법이 중요**함

Sampling
--------

### Temperature

* 낮은 값은 보수적이고 반복적이며, 높은 값은 **창의적인 결과**를 유도하지만 **오류 가능성 증가**함
* logits를 온도 값으로 나눠 확률 분포의 \*\*샤프함(집중도)\*\*을 조절함

### Presence Penalty

* 한 번이라도 등장한 토큰은 **재등장 가능성 감소**시킴
* **사용된 여부만 판단**하며, 등장 횟수는 고려하지 않음

### Frequency Penalty

* **등장 횟수에 비례하여 점수 감점**됨
* 자주 나온 단어일수록 더 큰 불이익을 받아 **다양성을 높이는 데 기여**함

### Repetition Penalty

* 이전에 등장한 토큰에 대해 **긍정/부정 logit에 비대칭 패널티 적용**함
* **루프성 반복 방지에 효과적**이지만 문맥의 일관성을 해칠 수 있음

### DRY (Don't Repeat Yourself)

* n-gram 패턴의 반복을 감지하여 **반복을 예측하는 토큰의 확률을 감점**함
* **길이가 길고 최근에 반복된 구절일수록 더 강하게 억제**함
* 창작 텍스트에서 **자연스러움을 유지하며 반복을 줄이는 데 탁월함**

### Top-K

* 상위 K개의 후보만 남기고 나머지 토큰은 제외함
* **극단적인 샘플을 제거**하면서도 일정 수준의 무작위성을 확보함

### Top-P (Nucleus Sampling)

* 누적 확률이 P 이상이 될 때까지의 토큰만 남기고 나머지는 제거함
* **모델 확신도에 따라 후보군 크기가 달라져 적응적임**

### Min-P

* 가장 높은 확률의 토큰을 기준으로 최소한의 비율 이상을 갖는 토큰만 남김
* **모델의 자신감에 따라 필터링이 동적으로 조정됨**

### Top-A

* 가장 확률 높은 토큰의 확률 제곱에 비례한 임계값으로 후보 필터링
* 확신이 높을수록 **더 엄격한 필터링이 적용**됨

### XTC

* 특정 확률로 **가장 확률 높은 후보들을 의도적으로 제거**하고 덜 확실한 선택을 하도록 유도
* **비정형적이거나 창의적인 응답을 위한 기법**

### Top-N-Sigma

* 확률 분포의 **표준편차 기준으로 유효 토큰을 선별**
* **통계적 특징에 기반한 필터링**으로 다양한 상황에 유연하게 대응함

### Tail-Free Sampling (TFS)

* 확률 기울기의 **이차 변화량(곡률)** 을 통해 유의미한 후보와 롱테일 후보를 구분함
* **자연스러운 컷오프 지점**을 찾아 필터링하는 방식

### Eta Cutoff

* 분포의 엔트로피(불확실성)에 따라 동적으로 필터 기준을 조정함
* **확신이 높을수록 더 많은 토큰이 제거되고, 확신이 낮을수록 유연함**

### Epsilon Cutoff

* **고정된 확률 임계값**을 사용해 낮은 확률 토큰 제거
* 단순하지만 예측 가능하고, **불필요한 롱테일 제거에 유용**

### Locally Typical Sampling

* **예상 surprisal(예측값과의 차이)** 이 평균에 가까운 토큰을 선호
* 가장 가능성 높은 토큰이나 가장 이상한 토큰 대신 **"전형적인" 선택**을 유도함
