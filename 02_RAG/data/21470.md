# Q-learning은 아직 확장 불가능함


* 최근 대형 언어 모델(LLM) 등에서처럼 **강화학습(RL)의 확장성**이 주목받고 있음
* 실제로 AlphaGo, LLM 등은 강력한 성능을 보이나 **주로 on-policy RL 알고리듬**이 사용되고 있음
* **Off-policy RL**의 대표 알고리듬인 **Q-learning**은 긴 문제(horizon)에서 **누적 편향** 문제로 인해 확장성이 떨어짐
* 실험 결과, 데이터와 컴퓨팅을 크게 늘려도 표준 Q-learning 계열 알고리듬은 복잡한 장기 과제에서 성능 한계가 존재함
* horizon 문제를 완화하는 **hierarchy 방법** 등 국소적 해법밖에 없어, **근본적으로 확장 가능한 새로운 오프폴리시 RL 목표**가 필요함

---

RL, 확장 가능한가?
------------

* 최근 **언어모델의 다음 토큰 예측, 확산모델, 대조학습 방식** 등은 데이터와 컴퓨트를 늘릴수록 잘 확장되는 목표임
* 게임, 수학, 코딩 등에서 RL 역시 강력한 성과가 있었으며, 그 중 많은 경우 **on-policy RL 알고리듬**(예: PPO, REINFORCE 등)이 활용됨
* On-policy RL은 항상 새로운 roll-out, 즉 **최신 정책으로 직접 생성한 데이터**만 사용 가능함
* 이러한 방식은 시뮬레이션이나 LLM에선 큰 문제가 아니나, **로봇 등 실제 환경에서 매우 비효율적**임
* 예를 들어, 로봇 실험에서 충분한 데이터를 얻기까지 **수개월**이 소요되고, 사람의 수동적 개입이 필요함

Off-policy RL의 등장
-----------------

* **Off-policy RL**은 **이전의 모든 데이터**를 재활용 가능하다는 점에서 sample efficiency가 뛰어남
* 대표적으로 **Q-learning**이 널리 쓰이고, 실시간 강아지 로봇 워킹 등 성과를 보임
* Q-learning은 **temporal difference(TD) loss** 최소화를 활용하며, 거의 모든 오프폴리시 RL이 이 원리를 따름
* 현실 문제에 RL을 적용하려면 결국 **Q-learning도 확장 가능한가?** 라는 질문이 핵심임

Q-learning의 확장 한계
-----------------

* 저자는 **현재 Q-learning은 긴 horizon(100 decision steps 이상) 문제가 등장하면 잘 확장되지 않음**을 주장함
* 여기서 “확장성”이란 **문제의 깊이/난이도(‘depth’)가 증가해도 데이터와 연산 자원 투입만으로 해결 가능한가**를 의미함
* 여러 논문에서 실험적으로 증명했듯, **단순히 처리 가능한 문제 수(‘width’)만 늘리는 것이 아님**
* 저자의 주장: **Q-learning 계열은 깊이축(difficulty)에서 확장성이 떨어지며, 알고리듬 혁신이 필수적임**
* 주요 근거는 두 가지임: 하나는 경험적 성공 사례 부재, 다른 하나는 최근 수행한 체계적 실험임

경험적 근거
------

* AlphaGo, AlphaZero, MuZero는 모두 **model-based, on-policy RL**로 TD-learning 계열이 아님
* OpenAI Five 역시 PPO 등 on-policy 방법임
* LLM용 RL도 대부분 **정책 그라디언트 계열 on-policy** 변종이 주류임
* Q-learning이나 유사 off-policy RL이 AlphaGo나 LLM급의 대규모 실제 성공 사례는 **거의 없음**
* 저자는 여러 논문 및 실무 사례 조사결과 Q-learning 기반 대형 성공 사례를 알지 못한다고 밝힘

Q-learning의 한계 원인: Horizon과 누적 편향
---------------------------------

* Q-learning은 **부트스트랩한(추정치로 예측값 생성) TD 타깃이 항상 편향됨**; 이러한 **편향이 time-horizon을 따라 누적**됨
* 반면, 토큰 예측, 확산, 대조학습 등 다른 확장성 높은 목표는 **예측 타깃에 누적 편향이 없음**
* Horizon(결정 길이)이 길어질수록, 누적된 오차로 인해 Q-learning의 성능 확장이 제한됨
* 이를 완화하려고 discount factor를 작게 설정하는 사례가 많음
* **Policy gradient** 등 on-policy 값 추정 방식은 GAE 등 기법 덕분에 horizon 문제 영향이 상대적으로 적음

실험을 통한 확장성 한계 검증
----------------

* 최근 논문에서 ultra-long horizon 과제를위해 **OGBench** 등에서 수천 step짜리 어려운 task를 설계
* 환경에서 "거의 무한대" 데이터와 강력한 모델, 표현 신경망 부담 완화 등 잡음 요인을 최소화함
* 기존 오프라인 RL(BC, IQL, CRL, SAC+BC 등) 모두 **초대형 데이터셋에서도 복잡한 task를 학습 못함**
* 데이터와 모델 크기, 학습시간, 하이퍼파라미터 등 모든 변수에 대해 ablation test를 했으나 성능 한계 극복 실패
* 단, **horizon(의사결정 길이)을 줄이는 기법**만이 확실히 성능 확장에 효과적이었음

Horizon 축소 기법의 효과
-----------------

* n-step return, 계층형(hierarchical) RL 등 **horizon 축소**만이 RL 스케일링에 결정적으로 효과를 보임
* horizon 축소는 단순 학습 가속화가 아니라 **최종 성능 자체도 획기적으로 향상**시킴
* 하지만 이런 방식은 **문제 근본 해결이 아니라 horizon을 상수배만큼 줄이는 데 그침**
* horizon curse를 해소할 **새로운 알고리듬 접근법**이 필요함

새로운 확장성 있는 오프폴리시 RL 목표의 필요
--------------------------

* 지금까지의 연구로 단순히 데이터/모델 사이즈만 늘려서는 horizon curse를 근본적으로 극복할 수 없음이 증명됨
* 궁극적으로는 **임의 길이의 장기 문제에도 확장 가능한 오프폴리시 RL 변종**이 필요함
* 이 목표가 실현되면 로봇, LLM, 다양한 의사결정 agent 등 더 폭넓은 실세계 문제 해결이 가능해질 것임

향후 연구 아이디어 및 제안
---------------

* 두 단계 계층(hierarchy)을 뛰어넘어 **임의 길이 horizon**에 대응할 수 있는 단순하고 확장 가능한 새로운 계층적 구조 제안 가능
* **모델기반 RL(model-based RL)** 은 감독학습 기반 모델링과 on-policy RL 융합을 통해 scalable 할 가능성이 있음
* **TD learning을 아예 배제**한 quasimetric RL, contrastive RL 등 새로운 계열 탐구도 유용할 수 있음
* 생성한 평가 환경 및 코드 오픈, 다양한 새로운 RL 알고리듬의 **스케일 테스트 벤치마크**로 활용 가능함

감사의 말
-----

* 논문 및 포스트에 협력/피드백을 제공한 여러 연구자들에게 감사 인사를 전함
* 본 내용은 [Horizon Reduction Makes RL Scalable] 논문 등에 기반하며, 저자 개인 의견임을 명시함
