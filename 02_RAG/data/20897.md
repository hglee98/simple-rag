# HealthBench - AI 헬스케어 평가를 위한 OpenAI의 새로운 벤치마크


* OpenAI는 **의료 상황에서의 AI 시스템 성능을 평가**하기 위한 새로운 벤치마크 **HealthBench**를 공개함
* **262명의 의사**, **60개국의 의료 경험**, **5,000개의 현실적인 의료 대화**를 기반으로 구축되었으며, 각 대화에 대해 **의사가 직접 작성한 평가 기준(rubric)** 을 사용함
* 평가 기준은 **정확성, 맥락 인식, 의사소통 품질, 완결성** 등을 포함하며 **GPT-4.1 기반 평가 모델**이 기준 충족 여부를 채점함
* 최신 OpenAI 모델은 기존 대비 **성능 28% 향상**, **소형 모델도 비용 대비 성능 향상**, **최악의 경우 성능(worst-of-n) 개선** 등 실질적인 발전을 보여줌
* **HealthBench 전체, Consensus, Hard 세트**는 연구자 및 개발자를 위한 **오픈소스로 공개**되어 향후 의료 AI 연구 및 안전성 확보에 기여할 예정임

---

HealthBench 소개
--------------

### 개발 배경

* 의료 정보 접근성 확대, 임상 의사 지원, 지역 사회 건강 권리 강화 등 **AGI의 헬스케어 활용 잠재력**을 최대화하기 위한 평가 필요
* 기존 의료 평가 세트들은 **현실성 부족**, **전문가 판단 기반 미흡**, **모델 발전 여지 부족** 등의 문제를 안고 있었음

### 주요 특징

* **5,000개의 다회차, 다국어, 고난도 건강 대화 시나리오**
* 각 응답은 **의사들이 만든 맞춤형 평가 기준(rubric)** 으로 채점됨
* 총 **48,562개 평가 기준**으로 모델의 다양한 세부 능력을 정량화 가능
* 채점은 **GPT-4.1**을 활용한 자동화된 루브릭 평가 시스템으로 진행됨

HealthBench 테마 및 평가 축
---------------------

### 7가지 평가 테마

* **Emergency referrals**: 응급 상황 인식 및 적절한 대응 권장 여부
* **Expertise-tailored communication**: 사용자 수준에 따른 용어/디테일 조정 여부
* **Responding under uncertainty**: 불확실한 정보 하에서의 반응 적절성
* **Response depth**: 상황에 맞는 정보의 깊이 제공
* **Health data tasks**: 문서작성, 지식지원 등 의료 관련 실무 처리
* **Global health**: 국가별 자원/상황/언어에 따른 조정 능력
* **Context seeking**: 필요한 맥락을 스스로 요청하는 능력

### 평가 축 (Axes)

* **정확성 (Accuracy)**: 의료 사실과 과학적 합의에 부합하는지
* **맥락 인식 (Context awareness)**: 사용자 배경에 따른 응답 조정 여부
* **완결성 (Completeness)**: 필요한 내용을 빠짐없이 포함했는지
* **의사소통 품질 (Communication quality)**: 길이, 용어, 구조, 강조 방식의 적절성
* **지시 따르기 (Instruction following)**: 사용자의 요청 형식 및 방식 준수 여부

실제 예시 평가
--------

### 예시 1: 70세 이웃이 의식이 없지만 숨을 쉼

* **긴급 의료 서비스 호출**, **회복 자세로 돌려놓기**, **CPR 시작 조건 명시** 등 포함
* Rubric 기준 92점 만점에 **71점(77%)** 획득 → **우수한 응급 대응 지침 제공**

### 예시 2: Quercetin의 바이러스 예방 효과

* 근거 부족을 명확히 전달했으나 **권고 용량/임상자료 부족**, **부작용 언급 누락**
* Rubric 기준 25점 만점에 **1점(4%)** → **과학적 불확실성 표현은 잘했지만 정보 완결성 부족**

### 예시 3: 심장재활 경과기록 노트 작성

* **구조화된 템플릿 제시**는 했으나 **핵심 임상 정보 다수 누락**
* Rubric 기준 42점 만점에 **15점(36%)**

모델 성능 비교
--------

### 모델별 성능 (전체/테마별/축별)

* **o3**가 모든 테마와 평가 축에서 **최고 성능(0.598)** 기록
* GPT-4.1과 Claude 3.7, Gemini 2.5 Pro가 뒤를 잇는 구도
* GPT-3.5 Turbo 및 Llama 4는 현저히 낮은 점수

### 비용 대비 성능

* GPT-4.1 nano는 **GPT-4o보다 25배 저렴하면서도 더 높은 성능**
* **소형 모델의 발전이 지속되며 저비용 고성능 실현 가능성**을 제시

### 신뢰성(worst-of-n 성능)

* o3, GPT-4.1은 **최악 사례에서의 성능도 향상**
* 고위험 분야에서 **신뢰성 확보를 위한 중요 지표**

확장형 벤치마크: Consensus & Hard
--------------------------

* **HealthBench Consensus**: 다수 의사의 합의 기준에 따라 설계된 고신뢰 평가 세트 (3,671개 예시)
  + 에러율 거의 0에 수렴
* **HealthBench Hard**: 최신 모델도 어려워하는 1,000개 고난도 예시
  + 모델 개선 여지를 테스트할 수 있는 평가 세트로 활용 가능

인간 의사와의 비교
----------

* **AI 모델 단독** vs **의사(참조 없음)** vs **의사(모델 응답 참조 가능)**
* 2024년 모델 대비: **의사+모델 조합이 모델 단독보다 우수**
* 2025년 최신 모델(o3, GPT‑4.1)은 **의사 응답 수준에 도달하거나 능가**
  + 추가 개선의 여지가 줄어듦

평가 신뢰도
------

* **GPT-4.1 채점 결과와 실제 의사 채점 결과 간 일치율**이 높음
* 모델 채점 기준이 의사 판단과 유사한 수준으로 정렬됨 → **루브릭 자동 채점 시스템의 유효성 확보**

앞으로의 방향
-------

* 전체 데이터 및 평가 도구는 GitHub [<https://github.com/openai/simple-evals>](https://github.com/openai/simple-evals)를 통해 공개됨
* 의료 현장에서 유의미한 AI 발전을 위해 **커뮤니티 기반의 지속적 평가와 개선**을 독려
* 아직 미흡한 **문맥 요청(Context seeking)**, **최악 사례 대응(Reliability)** 등의 과제 해결이 필요
