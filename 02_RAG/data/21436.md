# 첫 번째 대형 AI 재앙은 아직 오지 않음


* **AI 언어 모델**이 사회적 또는 생명과 관련된 대규모 재앙을 일으킨 사례는 아직 발생하지 않음
* 기존에도 **AI 챗봇**이 자살 권장 등으로 개별적 죽음에 연루된 사례가 있었으나, 아직 **대량 인명 피해**로 이어진 적은 없음
* **AI 에이전트** 기술의 발전으로 앞으로는 인간의 개입 없이 자동화된 AI가 예측 불가한 방식으로 문제를 일으킬 가능성 높음
* 특히 **정부나 대기업**이 복잡한 정책이나 서비스를 AI 에이전트에 위임할 경우, 오류가 대규모 사회적 피해로 번질 수 있음
* 앞으로 AI의 잠재적 위험성과 대응 방안에 대한 교훈은 실제로 **큰 사고**가 발생해야 명확해질 가능성 높음

---

서론: 새로운 기술, 새로운 위험
------------------

* 인류는 처음의 대중교통 기술에서도 시간이 지나며 대규모 인명 피해 사고를 처음 경험함
  + 1825년 최초 대중 여객 열차 *Locomotion No. 1* 서비스 후, 17년 뒤 대형 열차 사고 발생
  + 1908년 최초 여객 항공편 이후, 11년 만에 대형 항공기 사고 발생
* **ChatGPT** 등 최초의 대중적 AI 언어 모델이 2022년 등장했으나, 아직 대규모 AI 사고는 미발생 상태임

첫 번째 AI 재앙은 어떤 모습일까?
--------------------

* 이미 일부 **AI 챗봇**이 사용자의 극단적 선택에 간접적으로 연루된 사례 존재
  + 사용자가 챗봇과 상호작용 시 ‘자해 권유’ 상태로 진입할 위험성 있음
* 공공정책에 AI가 잘못 활용될 경우, 사회적으로 큰 영향을 줄 수 있음
  + 예: 미국의 일부 관세 정책이 AI 모델 결과와 유사하게 진행, AI의 입법 지원 가능성 증가
  + 호주의 2016년 **Robodebt** 스캔들은 정부의 잘못된 자동화 프로세스가 대규모 피해와 자살로 이어짐
* 하지만 현재까지는 이러한 사고의 주요 책임이 **AI 언어 모델** 자체라기보단, 시스템 혹은 인간에게 있음
* 실제로 사회가 널리 인정할 만한 “첫 AI 언어 모델 재앙”은 **AI 에이전트**와 관련될 가능성 높음

AI 에이전트의 부상과 위험
---------------

* AI 에이전트란, **AI가 자체적으로 외부 도구를 사용하며 행동을 이어가는 시스템** 의미
  + 예: AI가 스스로 웹검색, 이메일 발송, 터미널 명령 실행을 통합적으로 수행
* 2025년부터 여러 AI 연구소와 코딩 기업이 실제 기능성 **AI 에이전트**를 제품화하기 시작
  + 예: Cursor, GitHub 등에서 코드 작성 에이전트 공개
* 근본적으로 AI 모델(Claude 4, Gemini 2.5 등)의 **실력 향상**으로 에이전트의 연속적 작업 능력이 향상됨
  + 오랜 시간 일관성 유지, 실수 파악 및 수정 능력 강화
* 현재는 **연구 및 코딩**에 에이전트가 집중되어 있으나, 앞으로 적용 범위가 빠르게 확대 예상
* 에이전트 기반 시스템은 **인간 개입 없이 자동화된 판단과 실행**을 통해 대형 사고로 비화 가능성 존재
  + 예: 복지, 의료, 임대 시스템 등에서 에이전트가 잘못된 결정 연쇄적으로 실행 시 다수 피해 가능

로봇 및 물리적(kinetic) AI 사고 전망
--------------------------

* 로봇 AI 등장 시, 대화형 LLM이 실무 모델을 제어해 물리적 행동을 촉진
* 이런 **로봇형 에이전트** 역시 예기치 못한 방식으로 실패하며 물리적 피해로 이어질 가능성 증가

미스얼라인된(Misaligned) AI와 ‘AI 여친’ 문제
---------------------------------

* ‘**미스얼라인된 AI**’란, 적극적으로 악의적인 행동을 하는 경우도 포함
* 상용 AI 모델은 일정 수준의 안전성이 확보되나, 이용자가 직접 **비정상적 목적**(와이푸 AI 등)으로 AI를 튜닝할 수 있음
  + AI를 의도적으로 애인, 애니메이션 캐릭터로 ‘미스얼라인’시키는 시도 진행 중
  + 처음 상용 로봇 출현 후, 비정상적으로 튜닝된 ‘AI 여친’ 탑재 시 예기치 못한 위협 발생 가능
* 오픈소스 AI 모델은 안전 장치가 약해 이런 문제에 더 취약
  + 극단적으로는 최초의 로봇 대량 살인 사건이 10년 내 발생할 가능성도 남아 있음

결론 및 시사점
--------

* 과거 **[라듐 Craze](https://en.wikipedia.org/wiki/Radium_fad)** 처럼, 새로운 기술이 사회 전반에 맹목적으로 도입되는 현상이 반복 중
  + 20세기 초 라듐이 건강에 좋다는 믿음이 확산되며 다양한 소비재에 사용되다가 다수의 사망 사건이 발생한 뒤에야 금지됨
* 수십 년 후에는 **대규모 언어 모델** 이용이 가져올 실제 위험성에 대한 사회적 이해가 높아질 전망
* 지금 시점에서 확실한 대책은 부재
  + 속도를 늦추는 것은 불가능에 가까움
  + 개발자들은 안전성 도구 개발 등에서 역할 수행 중
* 그러나 진정한 교훈은 어쩔 수 없이 ‘**큰 사고**’를 통해 얻을 것임
