# Llama.cpp 이제 비전 기능 지원 (멀티모달 입력)


* **Llama.cpp**가 이제 **libmtmd**를 통해 멀티모달 입력(비전 포함)을 지원함
  + llama-mtmd-cli 또는 llama-server를 통한 OpenAI 호환 `/chat/completions` API
* Gemma 3, SmolVLM, Pixtral, Qwen 2/2.5, Mistra Small, InternVL 등 **모델**에서 멀티모달 기능 즉시 사용 가능함
  + Pre-quantized 모델들 제공 (대부분 QK\_K\_M 양자화를 기본 포함)
* 기본적으로 멀티모달 프로젝터는 **GPU**에 오프로딩되며, 필요시 비활성화 설정도 지원함
* 일부 모델은 **큰 컨텍스트 윈도우**(예: -c 8192)가 필요

---

개요
--

* **Llama.cpp**는 **libmtmd**를 이용해 멀티모달 입력 기능을 새롭게 지원하게 됨
* 사용자들은 이미지 등 텍스트 외 입력도 처리할 수 있게 되어 **비전 모델** 활용성이 확대됨
* 이 기능은 이미 Gemma 3, SmolVLM, Pixtral, Qwen 2 VL, Qwen 2.5 VL, Mistral Small, InternVL 등 주요 모델들과 호환됨

멀티모달 입력 활성화 방법
--------------

* 두 가지 주요 실행 방식이 안내됨: 첫 번째는 **-hf** 옵션 사용(지원 모델 필요), 두 번째는 **-m**과 **--mmproj** 옵션을 조합해 텍스트와 멀티모달 프로젝터 모델을 각각 지정하는 방법임
* **-hf 옵션** 사용 시, 멀티모달 기능을 끄고 싶으면 **--no-mmproj**를 추가하고, 사용자 지정 **mmproj 파일**을 활용할 경우 **--mmproj local\_file.gguf** 옵션을 사용함
* **GPU 오프로딩**이 기본값이며, 이를 원치 않으면 **--no-mmproj-offload** 옵션으로 비활성화가 가능함

예시 명령어
------

* 커맨드라인에서는 **llama-mtmd-cli**를, 서버에서는 **llama-server**를 활용하는 형태임
* 로컬 파일을 사용하는 경우 **--mmproj**로 직접 파일을 지정하는 방식임
* GPU 오프로딩을 비활성화하려면 **--no-mmproj-offload** 옵션을 추가 사용하는 방식임

즉시 사용 가능한 멀티모달 모델 목록
--------------------

* **Q4\_K\_M** 양자화를 기본으로 하는 다양한 준비된 모델들이 안내되어 있음
* 지원 모델 예시:
  + Gemma 3: 4b, 12b, 27b 버전
  + SmolVLM 계열: 256M, 500M, 2.2B 등
  + Pixtral 12B
  + Qwen 2 VL: 2B, 7B 및 Qwen 2.5 VL: 3B, 7B, 32B, 72B
  + Mistral Small 3.1 24B (IQ2\_M 양자화)
  + InternVL 2.5와 3 세대: 다양한 파라미터 크기 지원임

참고 사항
-----

* 사용 시 **(tool\_name)** 자리에 원하는 실행 바이너리 이름을 입력함(예: **llama-mtmd-cli** 또는 **llama-server**)
* 일부 멀티모달 모델 사용 시 **큰 컨텍스트 윈도우 크기** 지정이 필요할 수 있음(예: **-c 8192**와 같은 옵션 활용)
