# 동적 길이 부동소수를 통한 효율적인 GPU 추론을 위한 무손실 LLM 압축


* **DFloat11**은 대형 언어 모델(LLM)의 크기를 30% 줄이면서도 원본 모델과 비트 단위로 동일한 출력을 유지하는 **손실 없는 압축 프레임워크**임
* **BFloat16 가중치 표현의 낮은 엔트로피**를 활용하여 기존 저장 형식의 비효율성을 개선함
* **GPU에서 효율적인 추론**을 위해 **맞춤형 GPU 커널**을 개발하여 빠른 온라인 압축 해제를 지원함
* **Llama-3.1, Qwen-2.5, Gemma-3** 등의 최신 모델에서 실험을 통해 **30% 모델 크기 감소**와 **정확한 출력 유지**를 검증함
* **고정된 GPU 메모리 예산**으로 비압축 모델보다 **5.3-13.17배 긴 컨텍스트 길이**를 가능하게 함

---

70% 크기, 100% 정확도: 효율적인 GPU 추론을 위한 손실 없는 LLM 압축
----------------------------------------------

* 대형 언어 모델(LLM)의 크기가 급격히 증가하여 자원 제한 하드웨어에서의 효율적인 배포에 큰 도전이 됨
* \*\*Dynamic-Length Float (DFloat11)\*\*은 LLM의 크기를 30% 줄이면서도 비트 단위로 동일한 출력을 유지하는 손실 없는 압축 프레임워크임
* BFloat16 가중치 표현의 낮은 엔트로피를 활용하여 기존 저장 형식의 비효율성을 개선함
* 엔트로피 코딩을 적용하여 빈도에 따라 가중치에 동적 길이 인코딩을 할당하여 정보 최적의 압축을 달성함
* 효율적인 추론을 위해 맞춤형 GPU 커널을 개발하여 빠른 온라인 압축 해제를 지원함

DFloat11의 설계
------------

* 메모리 집약적인 조회 테이블(LUT)을 GPU SRAM에 맞는 **압축된 LUT**로 분해함
* **경량 보조 변수**를 사용하여 스레드 읽기/쓰기 위치를 조정하는 **2단계 커널**을 개발함
* **변환기 블록 수준의 압축 해제**를 통해 지연 시간을 최소화함

실험 결과
-----

* Llama-3.1, Qwen-2.5, Gemma-3 등의 최신 모델에서 DFloat11이 **30% 모델 크기 감소**와 **정확한 출력 유지**를 검증함
* 비압축 모델의 일부를 CPU로 오프로드하는 대안과 비교하여 **1.9-38.8배 높은 처리량**을 달성함
* 고정된 GPU 메모리 예산으로 비압축 모델보다 **5.3-13.17배 긴 컨텍스트 길이**를 가능하게 함

DFloat11의 장점
------------

* Llama-3.1-405B, 810GB 모델을 **8x80GB GPU**가 장착된 단일 노드에서 손실 없는 추론을 가능하게 함
* 코드와 모델은 **공개 URL**에서 제공됨
